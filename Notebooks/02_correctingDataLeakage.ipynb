{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyrillosishak/re-SkinCancer/blob/main/Notebooks/02_correctingDataLeakage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcfa7f7-81ea-4dd1-9030-741d45aa45c8"
      },
      "source": [
        "# Mitigating Data Leakage in Skin Cancer Classification with Transfer Learning\n",
        "\n",
        "`Skin Cancer Classification with Transfer Learning` paper that we are discussing uses HAM_10000 dataset.\n",
        "\n",
        "The HAM_10000 dataset is a widely used dataset for dermatological image classification, containing thousands of images of pigmented lesions. While it has been a valuable resource for training machine learning models, recent studies have highlighted significant issues with data leakage due to duplicate images within the dataset. This data leakage can lead to overly optimistic performance estimates and undermine the validity of research findings \\[2\\].\n",
        "\n",
        "In this notebook, we tackle the data leakage problem in the HAM_10000 dataset by addressing the duplicates within the dataset. Notably, insights from the survey paper “Leakage and the Reproducibility Crisis in ML-based Science” highlighted the issue of data leakage and its detrimental effects on reproducibility in machine learning. This survey also references the study “Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets,” which provided evidence of duplicate images in dermatological datasets, including `HAM_10000`\\[1\\].\n",
        "\n",
        "Our approach involves two main notebooks: the first reproduces the original paper, and the second focuses on identifying and addressing data leakage in the HAM_10000 dataset. By showing image similarity and identifying duplicates, we will clean the validation dataset and subsequently evaluate the impact on model accuracy and confusion metrics.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**🔍 In this notebook, we will:**\n",
        "\n",
        "1.  Identify Duplicates in `HAM_10000`4\n",
        "2.  Clean the Validation Dataset.\n",
        "3.  Evaluate Model Performance on new clean validation data.\n",
        "4.  Discuss Implications."
      ],
      "id": "1fcfa7f7-81ea-4dd1-9030-741d45aa45c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8639793-4d5f-48dd-b6a2-4ed97b499ba0"
      },
      "source": [
        "## **1. Identify Duplicates in HAM_10000**\n",
        "\n",
        "**`HAM_10000` have severe flaw :**\n",
        "\n",
        "> A caveat of HAM10000, despite its rather large size, is that it contains multiple images of the same lesion captured either from different viewing angles or at different magnification levels\n",
        "\n",
        "> the number of lesions with unique lesion IDs (HAM_xxx) is smaller than the number of images with unique image IDs (ISIC_xxx).\n",
        "\n",
        "> observe that the 10,015 images are in fact derived from only 7,470 unique lesions, and 1,956 of these lesion IDs (∼26.18%) contains 2 or more images: 1,423 lesions have 2 images, 490 lesions have 3 images, 34 lesions have 4 images, 5 lesions have 5 images, and 6 lesions have 4 images each.\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Near-duplicate_HAM10000.png\" height = 150> <img src = \"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Near-duplicate2_HAM10000.png\" height = 150>\n",
        "\n",
        "*This images from \\[2\\].*\n",
        "\n",
        "This results in near-duplicates within the dataset, which can compromise the integrity of any machine learning model trained on it. The presence of near-duplicate images can lead to data leakage, where the model learns to recognize specific lesions rather than generalizing to new, unseen lesions.\n",
        "\n",
        "*The duplicates were identified using a tool called [FastDup](https://github.com/visual-layer/fastdup) in the repository.*"
      ],
      "id": "f8639793-4d5f-48dd-b6a2-4ed97b499ba0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfd084f5-94a7-4062-84ba-5841b3b56038"
      },
      "outputs": [],
      "source": [
        "# Download duplicates file data\n",
        "!wget https://github.com/kakumarabhishek/Corrected-Skin-Image-Datasets/raw/main/DermaMNIST/HAM10000_DuplicateConfirmation/fastdup_outputs/duplicates_1000.csv"
      ],
      "id": "bfd084f5-94a7-4062-84ba-5841b3b56038"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0205de21-24fb-49d9-9d00-50f2e246baf0"
      },
      "outputs": [],
      "source": [
        "# Download the dataset to dir data\n",
        "!curl -L -O -J -H \"X-Dataverse-key:$API_TOKEN\" https://dataverse.harvard.edu/api/access/dataset/:persistentId/?persistentId=doi:10.7910/DVN/DBW86T\n",
        "!unzip -q dataverse_files.zip\n",
        "!mkdir data\n",
        "%cd data\n",
        "!unzip -q ../HAM10000_images_part_1.zip\n",
        "!unzip -q ../HAM10000_images_part_2.zip\n",
        "%cd .."
      ],
      "id": "0205de21-24fb-49d9-9d00-50f2e246baf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ceaf46e-7cb0-48f2-80d4-636325f6ab04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "duplicates = pd.read_csv(\"duplicates_1000.csv\")\n",
        "duplicates['from_img'] = duplicates['from_img'].str.replace('.jpg', '')\n",
        "duplicates['to_img'] = duplicates['to_img'].str.replace('.jpg', '')\n",
        "duplicates.head()"
      ],
      "id": "3ceaf46e-7cb0-48f2-80d4-636325f6ab04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76b46754-49cc-413b-b55f-94cc83f078c7"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file containing metadata for the HAM10000 dataset\n",
        "meta_data = pd.read_csv('HAM10000_metadata')\n",
        "\n",
        "# Group the image filenames by their class labels ('dx') and convert the groups to a dictionary\n",
        "# The dictionary keys are class labels and the values are lists of image filenames belonging to each class\n",
        "class_files = meta_data.groupby('dx')['image_id'].apply(list).to_dict()\n",
        "\n",
        "# Create a mapping from class names to integer indices\n",
        "# This is useful for converting class labels to numeric format for machine learning tasks\n",
        "label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
        "\n",
        "# Create a list of class names\n",
        "class_names = [class_name for class_name in class_files.keys()]\n",
        "\n",
        "print(\"Class names: \",class_names)\n",
        "print(\"Label map: \",label_map)"
      ],
      "id": "76b46754-49cc-413b-b55f-94cc83f078c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6755a343-772f-400b-9a9f-73c0af93ace4"
      },
      "source": [
        "## **2. Clean the data**\n",
        "\n",
        "In `HAM_10000` there is 2 problems in the data :\n",
        "\n",
        "1.  It contains multiple images of the same lesion captured either from different viewing angles or at different magnification levels\n",
        "\n",
        "    -   To fix this issue we will put the images with the same lesion_id in the train set and the rest of them will be in the validation set\n",
        "\n",
        "2.  It has a near-duplicates not from the same lesion\n",
        "\n",
        "    -   To fix this we will put the simialr images in train set and the rest of them will be in the validation set\n",
        "    -   There is some images (confusing images) that have high similarity have different labels so we removed them to not confuse the model"
      ],
      "id": "6755a343-772f-400b-9a9f-73c0af93ace4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19bf8cad-e2dd-467b-ab49-f6d947d59daf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "19bf8cad-e2dd-467b-ab49-f6d947d59daf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f286d38-b301-4789-8f79-9d33a03e6a74"
      },
      "source": [
        "### Grouping the similar images and confusing images:\n",
        "\n",
        "1.  If 2 similar images have different label we will append this image to `confusing_imgs` list.\n",
        "\n",
        "2.  else we will append this image to `similar_imgs` list."
      ],
      "id": "1f286d38-b301-4789-8f79-9d33a03e6a74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61bf8336-79ef-497a-be35-4dcf8e9692f7"
      },
      "outputs": [],
      "source": [
        "def get_confusing_and_similar_images(df,df_s):\n",
        "    \"\"\"\n",
        "    Identify confusing and similar images based on labels.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing image metadata.\n",
        "        df_s (pd.DataFrame): DataFrame containing similar images.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Lists of confusing and similar images.\n",
        "    \"\"\"\n",
        "    confusing_imgs, similar_imgs = [], []\n",
        "\n",
        "    for idx, row in df_s.iterrows():\n",
        "        from_img, to_img = row['from_img'], row['to_img']\n",
        "        if df[df['image_id'] == from_img]['dx'].values[0] != df[df['image_id'] == to_img]['dx'].values[0]:\n",
        "            confusing_imgs.extend([from_img, to_img])\n",
        "        elif from_img not in confusing_imgs and to_img not in confusing_imgs:\n",
        "            similar_imgs.extend([from_img, to_img])\n",
        "\n",
        "    return confusing_imgs, similar_imgs"
      ],
      "id": "61bf8336-79ef-497a-be35-4dcf8e9692f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bd28d44-540f-48c0-bea6-89a209325c94"
      },
      "outputs": [],
      "source": [
        "confusing_imgs, similar_imgs = get_confusing_and_similar_images(meta_data,duplicates)\n",
        "print(len(confusing_imgs),len(similar_imgs))"
      ],
      "id": "3bd28d44-540f-48c0-bea6-89a209325c94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a974d480-fce7-45eb-8129-acf30b4b5729"
      },
      "source": [
        "### Filtering images based on `confusing_imgs` and `similar_imgs` :\n",
        "\n",
        "*Iterating over all image list:*\n",
        "\n",
        "1.  If the image is a confusing image we will remove it.\n",
        "2.  If the image is not a confusing image but in similar images list we will put it in `middle_ground` list to sperate them from images that have no similarities and append this image lesion_id."
      ],
      "id": "a974d480-fce7-45eb-8129-acf30b4b5729"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd8be598-e328-4c11-8885-78de59a9d535"
      },
      "outputs": [],
      "source": [
        "def filter_images(image_list, confusing_imgs, similar_imgs, df, middle_ground, lesion_id_restricted):\n",
        "    \"\"\"\n",
        "    Filter out confusing images and separate similar ones.\n",
        "\n",
        "    Args:\n",
        "        image_list (list): List of image filenames.\n",
        "        confusing_imgs (list): List of confusing image filenames.\n",
        "        similar_imgs (list): List of similar image filenames.\n",
        "        df (pd.DataFrame): DataFrame containing image metadata.\n",
        "        middle_ground (list): List to store images that are similar but not confusing.\n",
        "        lesion_id_restricted (list): List to store lesion IDs of similar images.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Filtered image list, middle ground images, and lesion IDs of similar images.\n",
        "    \"\"\"\n",
        "    filtered_images = []\n",
        "    for img in image_list:\n",
        "        if img in confusing_imgs:\n",
        "            continue\n",
        "        elif img in similar_imgs:\n",
        "            middle_ground.append(img)\n",
        "            lesion_id_restricted.append(df[df['image_id'] == img]['lesion_id'].values[0])\n",
        "        else:\n",
        "            filtered_images.append(img)\n",
        "    return filtered_images, middle_ground, lesion_id_restricted"
      ],
      "id": "cd8be598-e328-4c11-8885-78de59a9d535"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3761f8-7089-4a0f-8a68-cf4c2ac92916"
      },
      "source": [
        "### How to detect duplicates"
      ],
      "id": "3b3761f8-7089-4a0f-8a68-cf4c2ac92916"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de30cd46-30fb-479d-9db4-3f3beb91a12e"
      },
      "outputs": [],
      "source": [
        "def check_duplicates(df_s, train_dataset, val_dataset):\n",
        "    \"\"\"\n",
        "    Check for duplicates between training and validation datasets.\n",
        "\n",
        "    Args:\n",
        "        df_s (pd.DataFrame): DataFrame containing similarity data.\n",
        "        train_dataset (SkinLesionDataset): Training dataset.\n",
        "        val_dataset (SkinLesionDataset): Validation dataset.\n",
        "\n",
        "    Returns:\n",
        "        list: List of duplicate image locations.\n",
        "    \"\"\"\n",
        "    duplicate_results = []\n",
        "    for index, row in df_s.iterrows():\n",
        "        from_img, to_img = row['from_img'], row['to_img']\n",
        "        from_img_in_train, from_img_in_val = from_img in train_dataset.image_list, from_img in val_dataset.image_list\n",
        "        to_img_in_train, to_img_in_val = to_img in train_dataset.image_list, to_img in val_dataset.image_list\n",
        "\n",
        "        if (from_img_in_train and to_img_in_val) or (from_img_in_val and to_img_in_train):\n",
        "            location = {\n",
        "                'from_img': from_img,\n",
        "                'to_img': to_img,\n",
        "                'from_img_location': 'train' if from_img_in_train else 'val',\n",
        "                'to_img_location': 'val' if to_img_in_val else 'train'\n",
        "            }\n",
        "            duplicate_results.append(location)\n",
        "\n",
        "    return duplicate_results"
      ],
      "id": "de30cd46-30fb-479d-9db4-3f3beb91a12e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f624def-8a06-4c0b-8e08-5787a20f7641"
      },
      "source": [
        "### Loading dataset"
      ],
      "id": "0f624def-8a06-4c0b-8e08-5787a20f7641"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aa4a339-a658-4960-87f6-c4618d7ba431"
      },
      "outputs": [],
      "source": [
        "class SkinLesionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for loading and transforming images of skin lesions along with their labels.\n",
        "\n",
        "    Args:\n",
        "        image_list (list of str): List of image filenames.\n",
        "        labels (list of int): List of labels corresponding to each image.\n",
        "        transform (callable, optional): Optional transform to be applied on an image sample.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_list, labels, transform=None):\n",
        "        self.image_list = image_list\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_list[idx]\n",
        "        img_path = \"./data/\"+img_path+\".jpg\"\n",
        "        image = PIL.Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ],
      "id": "1aa4a339-a658-4960-87f6-c4618d7ba431"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95d94ef7-0e5c-4c56-9f67-6d9610f3630b"
      },
      "outputs": [],
      "source": [
        "def process_train_val_loader(df, df_s, target_size, train_transform, val_transform):\n",
        "    \"\"\"\n",
        "    Processes and prepares the training and validation data loaders.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing image metadata.\n",
        "        df_s (pd.DataFrame): DataFrame containing similar and confusing images information.\n",
        "        target_size (int): The desired number of training images after augmentation.\n",
        "        train_transform (callable): Transformations to be applied to training images.\n",
        "        val_transform (callable): Transformations to be applied to validation images.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the training dataset, validation dataset, training data loader, and validation data loader.\n",
        "    \"\"\"\n",
        "    # Group image IDs by class (dx) and convert to dictionary\n",
        "    class_files = df.groupby('dx')['image_id'].apply(list).to_dict()\n",
        "    # Remove file extensions from the 'from_img' and 'to_img' columns in df_s\n",
        "    df_s['from_img'] = df_s['from_img'].str.replace('.jpg', '')\n",
        "    df_s['to_img'] = df_s['to_img'].str.replace('.jpg', '')\n",
        "    # Create a label map assigning an index to each class\n",
        "    label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
        "\n",
        "    # Get lists of confusing and similar images\n",
        "    confusing_imgs, similar_imgs = get_confusing_and_similar_images(df, df_s)\n",
        "\n",
        "    # Initialize lists to store training and validation images and labels\n",
        "    train_images, train_labels, val_images, val_labels = [], [], [], []\n",
        "\n",
        "    # Process each class to filter out confusing images and handle similar ones\n",
        "    for class_name, image_list in class_files.items():\n",
        "        middle_ground, lesion_id_restricted = [], []\n",
        "\n",
        "        # Filter out confusing images and categorize similar images\n",
        "        image_list, middle_ground, lesion_id_restricted = filter_images(\n",
        "            image_list, confusing_imgs, similar_imgs, df, middle_ground, lesion_id_restricted\n",
        "        )\n",
        "\n",
        "        # Assign labels to the images\n",
        "        labels = [label_map[class_name]] * len(image_list)\n",
        "        # Augment and split the data into training and validation sets\n",
        "        train_dataset, val_dataset = augment_and_split_data(\n",
        "            df, middle_ground, lesion_id_restricted, image_list, labels, train_transform, val_transform, target_size\n",
        "        )\n",
        "\n",
        "        # Extend the main lists with the processed data\n",
        "        train_images.extend(train_dataset.image_list)\n",
        "        train_labels.extend(train_dataset.labels)\n",
        "        val_images.extend(val_dataset.image_list)\n",
        "        val_labels.extend(val_dataset.labels)\n",
        "\n",
        "    # Create dataset objects for training and validation\n",
        "    train_dataset = SkinLesionDataset(train_images, train_labels, transform=train_transform)\n",
        "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
        "\n",
        "    # Create data loaders for training and validation datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "    return train_dataset, val_dataset, train_loader, val_loader\n",
        "\n",
        "\n",
        "def augment_and_split_data(df, middle_ground, lesion_id_restricted, image_list, label_list, train_transform, val_transform, target_size=300):\n",
        "    \"\"\"\n",
        "    Augments and splits a dataset of images and labels into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing 'image_id' and 'lesion_id' columns.\n",
        "        middle_ground (list): List of images that are similar but not confusing.\n",
        "        lesion_id_restricted (list): List of lesion IDs of similar images.\n",
        "        image_list (list of str): List of image filenames.\n",
        "        label_list (list of int): List of labels corresponding to each image.\n",
        "        train_transform (callable): Transformations to be applied to training images.\n",
        "        val_transform (callable): Transformations to be applied to validation images.\n",
        "        target_size (int, optional): Desired number of training images after augmentation. Default is 300.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the training dataset and validation dataset.\n",
        "    \"\"\"\n",
        "    # Combine images and labels and shuffle them\n",
        "    combined = list(zip(image_list, label_list))\n",
        "    random.shuffle(combined)\n",
        "    image_list[:], label_list[:] = zip(*combined)\n",
        "\n",
        "    # Create a dictionary mapping image IDs to lesion IDs\n",
        "    lesion_dict = df.set_index('image_id')['lesion_id'].to_dict()\n",
        "    # Initialize a dictionary to group images by lesion ID\n",
        "    lesion_groups = {lesion_id: [] for lesion_id in lesion_dict.values()}\n",
        "\n",
        "    # Group images by lesion ID\n",
        "    for img, lbl in combined:\n",
        "        lesion_id = lesion_dict[img]\n",
        "        lesion_groups[lesion_id].append((img, lbl))\n",
        "\n",
        "    # Initialize lists to store training and validation images and labels\n",
        "    train_images_labels, val_images_labels = [], []\n",
        "\n",
        "    # Distribute images into training and validation sets\n",
        "    for lesion_id, imgs_labels in lesion_groups.items():\n",
        "        if len(val_images_labels) + len(imgs_labels) <= 0.2 * len(image_list) and lesion_id not in lesion_id_restricted:\n",
        "            val_images_labels.extend(imgs_labels)\n",
        "        else:\n",
        "            train_images_labels.extend(imgs_labels)\n",
        "\n",
        "    # Add middle ground images to the training set\n",
        "    train_images_labels += [(img, label_list[0]) for img in middle_ground]\n",
        "\n",
        "    # Separate images and labels for training and validation sets\n",
        "    train_images, train_labels = zip(*train_images_labels)\n",
        "    val_images, val_labels = zip(*val_images_labels)\n",
        "\n",
        "    # Limit the number of images in the training and validation sets\n",
        "    if len(train_images) > 360:\n",
        "        train_images = train_images[:360]\n",
        "        train_labels = train_labels[:360]\n",
        "    if len(val_images) > 90:\n",
        "        val_images = val_images[:90]\n",
        "        val_labels = val_labels[:90]\n",
        "\n",
        "    # Augment the training images to reach the target size\n",
        "    augmented_images, augmented_labels = [], []\n",
        "    while len(augmented_images) < target_size:\n",
        "        for img, label in zip(train_images, train_labels):\n",
        "            augmented_images.append(img)\n",
        "            augmented_labels.append(label)\n",
        "            if len(augmented_images) >= target_size:\n",
        "                break\n",
        "\n",
        "    # Create dataset objects for augmented training data and validation data\n",
        "    train_dataset = SkinLesionDataset(augmented_images, augmented_labels, transform=train_transform)\n",
        "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
        "\n",
        "    return train_dataset, val_dataset\n"
      ],
      "id": "95d94ef7-0e5c-4c56-9f67-6d9610f3630b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0374aeed-f7d0-4c91-bbcb-27644f8ef11e"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomAffine(\n",
        "        degrees=30,\n",
        "        translate=(0.1, 0.1),\n",
        "        scale=None,\n",
        "        shear=10\n",
        "    ),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "id": "0374aeed-f7d0-4c91-bbcb-27644f8ef11e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6216e412-853a-49c9-a53a-15b6b3565378"
      },
      "outputs": [],
      "source": [
        "meta_data = pd.read_csv('HAM10000_metadata')\n",
        "duplicates = pd.read_csv(\"duplicates_1000.csv\")"
      ],
      "id": "6216e412-853a-49c9-a53a-15b6b3565378"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb2b735f-5e47-448b-9bbb-3ac942e62fa5"
      },
      "outputs": [],
      "source": [
        "number = 1000\n",
        "num_epochs = 100\n",
        "patience = 15\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dataset, val_dataset, train_loader, val_loader = process_train_val_loader(meta_data, duplicates, number, train_transform, val_transform)\n",
        "print(\"Size of trainset : \" + str(len(train_dataset.image_list)))\n",
        "print(\"Size of validationset : \" + str(len(val_dataset.image_list)))"
      ],
      "id": "eb2b735f-5e47-448b-9bbb-3ac942e62fa5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d00d22ff-f01e-4c94-a908-1797b9a795aa"
      },
      "outputs": [],
      "source": [
        "duplicate_results = check_duplicates(duplicates, train_dataset, val_dataset)\n",
        "print(len(duplicate_results))"
      ],
      "id": "d00d22ff-f01e-4c94-a908-1797b9a795aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545861c7-836e-46f3-8254-e340f4ef93c6"
      },
      "source": [
        "After cleaning the data we should use the code in [Reproducing the original paper notebook](Markdowns/01-reproducingSkinCancer.md) to train the model on the clean data."
      ],
      "id": "545861c7-836e-46f3-8254-e340f4ef93c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df894df5-b00d-461f-af44-9d83ec993657"
      },
      "source": [
        "## **3. Evaluate Model Performance on new clean validation data**\n",
        "\n",
        "1.  Downloading the model trained on the cleaned data and the model trained on duplicated data.\n",
        "2.  Downloading the datasets that is used in every model.\n",
        "3.  Evaluate each model"
      ],
      "id": "df894df5-b00d-461f-af44-9d83ec993657"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba3028f7-b963-4b39-8e8b-2bf33f1f73ff"
      },
      "outputs": [],
      "source": [
        "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp3/val_loader_clean.pt\n",
        "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp2/val_loader.pt"
      ],
      "id": "ba3028f7-b963-4b39-8e8b-2bf33f1f73ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d59f278-443e-479c-b19d-39af70936819"
      },
      "outputs": [],
      "source": [
        "val_loader_clean = torch.load('val_loader_clean.pt')\n",
        "val_loader = torch.load('val_loader.pt')"
      ],
      "id": "7d59f278-443e-479c-b19d-39af70936819"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b51c82d-499e-4f22-8f54-d45373869e5d"
      },
      "outputs": [],
      "source": [
        "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp3/1000_clean.pt\n",
        "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp2/1000.pt"
      ],
      "id": "7b51c82d-499e-4f22-8f54-d45373869e5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5858bf04-75b1-447a-b72d-4c65efb8f433"
      },
      "outputs": [],
      "source": [
        "class ModifiedInceptionResNetV2(nn.Module):\n",
        "    \"\"\"ModifiedInceptionResNetV2 class for transfer learning with custom classifier.\n",
        "\n",
        "      This class implements a modified version of the Inception ResNet V2 model for image classification tasks.\n",
        "      It leverages transfer learning by freezing the pre-trained feature extraction layers from a\n",
        "      provided Inception ResNet V2 model and adding a custom classifier on top.\n",
        "\n",
        "      Args:\n",
        "          original_model (torchvision.models.InceptionV3): A pre-trained Inception ResNet V2 model\n",
        "              (typically loaded with `pretrained=True`).\n",
        "          num_classes (int, optional): The number of output classes for the classification task.\n",
        "              Defaults to 7.\n",
        "\n",
        "      Attributes:\n",
        "          features (nn.Sequential): A sequential container holding all layers from the original model\n",
        "              except the final classifier (Softmax layer).\n",
        "          classifier (nn.Sequential): A custom classifier consisting of:\n",
        "              - nn.Flatten(): Flattens the input from the feature extractor.\n",
        "              - nn.Linear(1536, 64): First fully-connected layer with 64 units and ReLU activation.\n",
        "              - nn.Linear(64, num_classes): Second fully-connected layer with 'num_classes' units\n",
        "                and Softmax activation for probability distribution of the classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, original_model, num_classes=7):\n",
        "        super(ModifiedInceptionResNetV2, self).__init__()\n",
        "\n",
        "        # Retain all layers except the final classifier(Softmax)\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
        "\n",
        "        # Custom classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # 1536 output from the last layer after removing the classifier\n",
        "            nn.Linear(1536, 64),  # First fully connected layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes),  # Second fully connected layer\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "id": "5858bf04-75b1-447a-b72d-4c65efb8f433"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2783fa0e-28af-487c-960b-a2e31c83d204"
      },
      "outputs": [],
      "source": [
        "!pip -q install timm"
      ],
      "id": "2783fa0e-28af-487c-960b-a2e31c83d204"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e12073dc-be15-4553-acec-cd3a07e1ff45"
      },
      "outputs": [],
      "source": [
        "from timm import create_model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model('inception_resnet_v2', pretrained=True,num_classes=7)\n",
        "modified_model = ModifiedInceptionResNetV2(model, num_classes=7)\n",
        "\n",
        "clean_model = copy.deepcopy(modified_model)\n",
        "Dataleaked_model = copy.deepcopy(modified_model)\n",
        "\n",
        "clean_model.load_state_dict(torch.load('1000_clean.pt')['model'])\n",
        "Dataleaked_model.load_state_dict(torch.load('1000.pt')['model'])\n",
        "\n",
        "clean_model.to(device)\n",
        "Dataleaked_model.to(device)\n",
        "\n",
        "clean_model.eval()\n",
        "Dataleaked_model.eval()\n",
        "print(\"Loaded model successfully.\")"
      ],
      "id": "e12073dc-be15-4553-acec-cd3a07e1ff45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9d6a1de-f77a-4db1-9677-553190a28e4d"
      },
      "outputs": [],
      "source": [
        "def get_predictions_and_labels(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)"
      ],
      "id": "d9d6a1de-f77a-4db1-9677-553190a28e4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e796f3ca-530f-461e-b65c-cd2a8bab8b1a"
      },
      "outputs": [],
      "source": [
        "val_preds_duplicates, val_labels_duplicates = get_predictions_and_labels(Dataleaked_model, val_loader, device)\n",
        "val_preds_clean, val_labels_clean = get_predictions_and_labels(clean_model, val_loader_clean, device)"
      ],
      "id": "e796f3ca-530f-461e-b65c-cd2a8bab8b1a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8346ad6-f0b0-42ea-8f3f-a6c707256802"
      },
      "outputs": [],
      "source": [
        "accuracy_duplicates = np.mean(val_preds_duplicates == val_labels_duplicates)*100\n",
        "accuracy_clean = np.mean(val_preds_clean == val_labels_clean)*100\n",
        "print(f\"Validation Accuracy of the duplicated set: {accuracy_duplicates:.4f}\")\n",
        "print(f\"Validation Accuracy of the clean set: {accuracy_clean:.4f}\")"
      ],
      "id": "e8346ad6-f0b0-42ea-8f3f-a6c707256802"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602f6581-e556-4835-9ce3-21fd5d7f34a4"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ],
      "id": "602f6581-e556-4835-9ce3-21fd5d7f34a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fde3f8fa-7e9d-4868-84d5-c2a732a9c01e"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm1 = confusion_matrix(val_labels_duplicates, val_preds_duplicates)\n",
        "cm2 = confusion_matrix(val_labels_clean, val_preds_clean)"
      ],
      "id": "fde3f8fa-7e9d-4868-84d5-c2a732a9c01e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41b64043-93f8-4bb4-8d57-c6230b6d870a"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(cm1, class_names)"
      ],
      "id": "41b64043-93f8-4bb4-8d57-c6230b6d870a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69d30d57-af5b-4693-8525-bd9209ae8071"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(cm2, class_names)"
      ],
      "id": "69d30d57-af5b-4693-8525-bd9209ae8071"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a3a5827-e1ff-4540-aa14-6ec8f4f74fc3"
      },
      "source": [
        "As we see here, the model tested on images with near-duplicates in the training set achieves higher accuracy than the model tested on clean data, with a validation accuracy of 78.8703% compared to 73.5099%. This discrepancy highlights a significant issue : the presence of near-duplicate images in datasets can artificially inflate performance metrics, leading to misleading conclusions about a model’s efficacy. This has implications on the reproducibility and reliability of published research. Papers that report inflated performance metrics due to such data issues may set unrealistic benchmarks, making it difficult for subsequent researchers to replicate results or make fair comparisons. To ensure the integrity and credibility of machine learning research, it is crucial to rigorously check for and address near-duplicates and other forms of data leakage before publishing results."
      ],
      "id": "5a3a5827-e1ff-4540-aa14-6ec8f4f74fc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3cfe3c-4663-4b2f-a2a1-d980039247f8"
      },
      "source": [
        "## **Refrences**\n",
        "\n",
        "1.  [Leakage and the Reproducibility Crisis in ML-based Science](https://arxiv.org/abs/2207.07048)\n",
        "\n",
        "2.  [Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Dataset](https://arxiv.org/pdf/2401.14497)"
      ],
      "id": "be3cfe3c-4663-4b2f-a2a1-d980039247f8"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  }
}