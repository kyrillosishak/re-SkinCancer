{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Classification using Inception Network and Transfer Learning\n",
    "\n",
    "In this series of notebooks, we aim to reproduce the results of [Skin Cancer Classification using Inception Network and Transfer Learning](https://arxiv.org/pdf/2111.02402v1). The paper explores the application of advanced neural network architectures, specifically Inception Networks, in conjunction with transfer learning techniques for skin cancer classification. Our objective is to rigorously validate the findings of the paper, ensuring transparency and reproducibility in our approach. Additionally, we will investigate and mitigate potential sources of data leakage within the dataset. Data leakage can lead to overly optimistic results by inadvertently incorporating information from the validation or test sets into the training process. By identifying and addressing these issues, we aim to demonstrate how such leakage can impact model performance and interpretation of results.\n",
    "\n",
    "**ðŸ† Objectives of this notebooks:**\n",
    "\n",
    "1.  Identify the specific claims related to classification accuracy, generalization across different skin lesion types, and the efficacy of the proposed methodology.\n",
    "\n",
    "2.  Assess the methodology, dataset (HAM_10000), and the problem statement presented.\n",
    "\n",
    "3.  Define specific experiments needed to validate each identified claim.\n",
    "\n",
    "4.  Obtain the HAM_10000 dataset and perform data cleaning and preprocessing steps as mentioned in the paper.\n",
    "\n",
    "5.  Choose the same or similar models used in the paper for skin cancer classification and train the models using the preprocessed data and evaluate them using the same metrics reported in the paper.\n",
    "\n",
    "6.  Compare the results obtained from your reproduced models with the results reported in the original paper and analyze any differences or discrepancies in performance and identify potential reasons for variations.\n",
    "\n",
    "7.  Identify potential sources of data leakage within the HAM_10000 dataset.\n",
    "\n",
    "8.  Re-train models after mitigating identified data leakage sources and compare performance metrics before and after leakage removal.\n",
    "\n",
    "**ðŸ” In this notebook, we will:** 1. Identify the specific claims. 2. Define specific experiments. 3. Obtain the HAM_10000 dataset and perform data preprocessing. 4. Use the same model used in the paper for skin cancer using the preprocessed data. 5. Compare the results obtained from your reproduced models with the original paperâ€™s results. â€”\n",
    "\n",
    "**ðŸ—£ï¸ Claims :**\n",
    "\n",
    "1.  The model achieved a validation accuracy of 73.4% after the first training phase and 78.9% after the second training phase.\n",
    "2.  The results showed that the model could classify six out of seven categories with a true positive rate higher than 75%, even for classes with limited samples.\n",
    "3.  The entire training process required less than 20 GB of RAM and was completed in under four hours using a Google Colab GPU.\n",
    "\n",
    "**ðŸ§ªExperiment from the paper:**\n",
    "\n",
    "> Images are loaded and resized from 450Ã—600 to 299Ã—299 in order to be correctly processed by the network. After a normalization step on RGB arrays, we split the dataset into a training and validation set with 80:20 ratio.\n",
    "\n",
    "> In order to re-balance the dataset, we chose to shrink the amount of images for each class to an equal maximum dimension of 450 samples. This significant decrease of available images is then mitigated by applying a step of data augmentation. Training set expansion is made by altering images with small transformations to reproduce some variations, such as horizontal flips, vertical flips, translations, rotations and shearing transformations.\n",
    "\n",
    "> We decided to take advantage of transfer learning, utilizing `Inception-ResNet-v2` pre-trained on ImageNet.\n",
    "\n",
    "> The original `Inception-ResNet-v2` architecture has a stem block consisting of the concatenation of multiple convolutional and pooling layers, while Inception-ResNet blocks (A, B and C) contain a set of convolutional filters with an average pooling layer. *This structure has been extended with a final module consisting of a flattening step, two fully-connected layers of 64 units each, and the softmax classifier.*\n",
    "\n",
    "> In this work we used a stochastic gradient descent optimizer (SGD), with learning rate set to 0.0006 and usage of momentum and Nesterov Accelerated Gradient in order to adapt updates to the slope of the loss function (categorical cross entropy) and speed up the training process.\n",
    "\n",
    "> The total number of epochs was set to 100, using a small batch size of 10. A maximum patience of 15 epochs was set to the early stopping callback in order to mitigate the overfitting.\n",
    "\n",
    "> In order to improve classification performance, specially on minority classes, we loaded the best model obtained in the first round to extend the training phase and explore other potential local minimum points of the loss function, by using an additional amount of 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.Data Loading and preprocessing**\n",
    "\n",
    "`HAM10000`(Human Against Machine with 10000 images) contains 10,015 dermoscopic images of pigmented skin lesions collected from patients at two study sites in Australia and Austria, with their diagnoses confirmed by either histopathology, confocal microscopy, clinical follow-up visits, or expert consensus. The 7 disease labels in the dataset cover 95% of the lesions encountered in clinical practice. Because of these meritorious properties, `HAM10000` is a good candidate dataset for dermatological analysis. However the resulting `HAM10000` from serious flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading the data\n",
    "\n",
    "To download the data you should :\n",
    "\n",
    "1.  Signup https://dataverse.harvard.edu/\n",
    "2.  Create API token from [this](https://dataverse.harvard.edu/dataverseuser.xhtml?selectTab=apiTokenTab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export API_TOKEN=#[put your API token here without space between '=' and token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data from official Harvard dataverse website\n",
    "!curl -L -O -J -H \"X-Dataverse-key:$(API_TOKEN)\" https://dataverse.harvard.edu/api/access/dataset/:persistentId/?persistentId=doi:10.7910/DVN/DBW86T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q dataverse_files.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping all images in directory data\n",
    "!mkdir data\n",
    "%cd data\n",
    "!unzip -q ../HAM10000_images_part_1.zip\n",
    "!unzip -q ../HAM10000_images_part_2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Loading metadata\n",
    "df = pd.read_csv('../HAM10000_metadata')\n",
    "class_files = df.groupby('dx')['image_id'].apply(list).to_dict()\n",
    "print(\"This is the classes dictionary :\",class_files.keys())\n",
    "print(\"The number of images in class df    =\",len(class_files['df']))\n",
    "print(\"The number of images in class vasc  =\",len(class_files['vasc']))\n",
    "print(\"The number of images in class akiec =\",len(class_files['akiec']))\n",
    "print(\"The number of images in class bcc   =\",len(class_files['bcc']))\n",
    "print(\"The number of images in class bkl   =\",len(class_files['bkl']))\n",
    "print(\"The number of images in class mel   =\",len(class_files['mel']))\n",
    "print(\"The number of images in class nv    =\",len(class_files['nv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the data is heavily unbalanced. To achieve a balanced dataset, each class should have an average of 10015/7 â‰ˆ 1430 images. However, 6 out of the 7 classes fall below this number.\n",
    "\n",
    "Additionally, the authors made a mistake by not thoroughly exploring the data to identify that it contains duplicates. This oversight can lead to data leakage, negatively impacting model performance and evaluation. In the next notebook, we will identify this problem and take steps to address it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "# Choosing a random image\n",
    "list_of_images_paths = glob('../data/*')\n",
    "random_index = random.randint(0, len(list_of_images_paths) - 1)\n",
    "random_image_path = list_of_images_paths[random_index]\n",
    "# Load the image\n",
    "image = Image.open(random_image_path)\n",
    "print(\"Size of the image is\",image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Augmentation and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation\n",
    "\n",
    "> Images are loaded and resized from 450Ã—600 to 299Ã—299\n",
    "\n",
    "> Training set expansion is made by altering images with small transformations to reproduce some variations, such as horizontal flips, vertical flips, translations, rotations and shearing transformations.\n",
    "\n",
    "**If you are not fimiliar with torchvision transforms :**\n",
    "\n",
    "-   `transforms.Compose`: This function takes a list of transformations and applies them sequentially to an image.\n",
    "\n",
    "-   `transforms.RandomHorizontalFlip()` and `transforms.RandomVerticalFlip()` : This transformation randomly flips the image horizontally/vertically with a probability of 50%.\n",
    "\n",
    "-   `transforms.RandomAffine()` :\n",
    "\n",
    "    -   Randomly rotates the image within the range of -30 to +30 degrees.\n",
    "    -   Randomly translates the image horizontally and vertically by up to 10% of the image size.\n",
    "    -   Randomly applies a shear transformation within the range of -10 to +10 degrees.\n",
    "\n",
    "-   `transforms.ToTensor()`: This transformation converts a PIL Image or a numpy.ndarray (H x W x C) in the range \\[0, 255\\] to a torch.FloatTensor of shape (C x H x W) in the range \\[0.0, 1.0\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=30,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=None,\n",
    "        shear=10\n",
    "    ),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Dataset\n",
    "\n",
    "If you are not familiar with PyTorch Dataset & Dataloader [visit](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading and transforming images of skin lesions along with their labels.\n",
    "\n",
    "    Args:\n",
    "        image_list (list of str): List of image filenames.\n",
    "        labels (list of int): List of labels corresponding to each image.\n",
    "        transform (callable, optional): Optional transform to be applied on an image sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_list, labels, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_list[idx]\n",
    "        img_path = \"../data/\"+img_path+\".jpg\"\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data & creating data loaders\n",
    "\n",
    "> we split the dataset into a training and validation set with 80:20 ratio.\n",
    "\n",
    "> In order to re-balance the dataset, we chose to shrink the amount of images for each class to an equal maximum dimension of 450 samples. This significant decrease of available images is then mitigated by applying a step of data augmentation. Training set expansion is made by altering images with small transformations to reproduce some variations, such as horizontal flips, vertical flips, translations, rotations and shearing transformations.\n",
    "\n",
    "**The functions do the following :**\n",
    "\n",
    "-   it combines the image_list and label_list into pairs and shuffles them to ensure randomness. It then limits the size of the dataset to 450 images if the original list exceeds this number. The function splits the class size into training and validation sets, with the validation set comprising 20% of the total images.\n",
    "\n",
    "-   the function focuses on augmenting the training data to increase the dataset size up to a target size (default is 300). It repeatedly adds images from the training set until the desired target size is reached.\n",
    "\n",
    "-   the function creates and returns two dataset objects, train_dataset and val_dataset, using the SkinLesionDataset class, applying the specified transformations for training and validation respectively. This function is intended to be called for each of the seven classes individually, thus preparing the data in well-augmented manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_split_data(image_list, label_list, train_transform, val_transform, target_size=300):\n",
    "    \"\"\"\n",
    "    Augments and splits a dataset of images and labels into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        image_list (list of str): List of image filenames.\n",
    "        label_list (list of int): List of labels corresponding to each image.\n",
    "        train_transform (callable): Transformations to be applied to training images.\n",
    "        val_transform (callable): Transformations to be applied to validation images.\n",
    "        target_size (int, optional): Desired number of training images after augmentation. Default is 300.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Combine images and labels, and shuffle them\n",
    "    combined = list(zip(image_list, label_list))\n",
    "    random.shuffle(combined)\n",
    "    image_list[:], label_list[:] = zip(*combined)\n",
    "\n",
    "    # Limit the dataset size to 450 images if it exceeds this number\n",
    "    if len(image_list) > 450:\n",
    "        image_list = image_list[:450]\n",
    "        label_list = label_list[:450]\n",
    "\n",
    "    # Calculate the size of the validation set (20% of the total dataset)\n",
    "    val_size = math.ceil(0.2 * len(image_list))\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_images = image_list[:val_size]\n",
    "    val_labels = label_list[:val_size]\n",
    "    train_images = image_list[val_size:]\n",
    "    train_labels = label_list[val_size:]\n",
    "\n",
    "\n",
    "    # Augment the training set to reach the target size\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    while len(augmented_images) < target_size:\n",
    "        for img, label in zip(train_images, train_labels):\n",
    "            augmented_images.append(img)\n",
    "            augmented_labels.append(label)\n",
    "            if len(augmented_images) >= target_size:\n",
    "                break\n",
    "    train_dataset = SkinLesionDataset(augmented_images, augmented_labels, transform=train_transform)\n",
    "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_val_loader(target_size, train_transform, val_transform):\n",
    "    \"\"\"\n",
    "    Processes and prepares the training and validation data loaders.\n",
    "\n",
    "    Args:\n",
    "        target_size (int): The desired number of training images after augmentation.\n",
    "        train_transform (callable): Transformations to be applied to training images.\n",
    "        val_transform (callable): Transformations to be applied to validation images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training dataset, validation dataset, training data loader, and validation data loader.\n",
    "    \"\"\"\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "\n",
    "    # Create a label map to convert class names to numeric labels\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
    "\n",
    "    # Process each class in the class_files dictionary\n",
    "    for class_name, image_list in class_files.items():\n",
    "        # Generate labels for the current class\n",
    "        labels = [label_map[class_name]] * len(image_list)\n",
    "        # Augment and split the data into training and validation sets\n",
    "        train_dataset, val_dataset = augment_and_split_data(image_list, labels, train_transform, val_transform, target_size)\n",
    "        train_images.extend(train_dataset.image_list)\n",
    "        train_labels.extend(train_dataset.labels)\n",
    "        val_images.extend(val_dataset.image_list)\n",
    "        val_labels.extend(val_dataset.labels)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = SkinLesionDataset(train_images, train_labels, transform=train_transform)\n",
    "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "    return train_dataset, val_dataset, train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Model creation and Transfer Learning**\n",
    "\n",
    "> The original Inception-ResNet-v2 architecture has a stem block consisting of the concatenation of multiple convolutional and pooling layers, while Inception-ResNet blocks (A, B and C) contain a set of convolutional filters with an average pooling layer. This structure has been extended with a final module consisting of a flattening step, two fully-connected layers of 64 units each, and the softmax classifier.\n",
    "\n",
    "`Inception-ResNet-v2` is a deep convolutional neural network architecture that combines the strengths of two powerful designs: Inception and Residual Networks (ResNet). Inception modules aim to capture multi-scale features by performing convolutions of different sizes (e.g., 1x1, 3x3, 5x5) in parallel, followed by concatenation of the results. This design helps in efficiently capturing spatial hierarchies and reduces computational cost. Inspired by ResNet, Inception-ResNet-v2 incorporates skip (residual) connections which addresses the vanishing gradient problem and enabling the training of much deeper networks.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Original_Inception-ResNet-v2.png\" />\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Modified_Inception-ResNet-v2.png\" width=\"199\" height=\"469\" />\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Original model\n",
    "</td>\n",
    "<td>\n",
    "Modified model\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**Transfer Learning :**\n",
    "\n",
    "-   weâ€™ll leverage transfer learning to create a custom image classifier. Weâ€™ll be using the Inception ResNet V2 model pre-trained on ImageNet, a massive dataset with thousands of image classes. Transfer learning allows us to reuse the knowledge this model has learned from ImageNet, even if our own dataset has different categories.\n",
    "-   Weâ€™ll use the timm library from Hugging Face to load the Inception ResNet V2 model pre-trained on ImageNet. This pre-trained model has already learned effective ways to extract features from images.\n",
    "-   Weâ€™ll remove the final classification layer of the pre-trained model and add our own custom classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are in colab install timm \n",
    "!pip -q install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are in Kaggle notebook install torchsummary \n",
    "!pip -q install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from timm import create_model\n",
    "model = create_model('inception_resnet_v2', pretrained=True,num_classes=7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Inspect model parameters and layers\n",
    "summary(model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see these are the last 2 layers :\n",
    "\n",
    "    Dropout-1151                 [-1, 1536]               0\n",
    "    Linear-1152                    [-1, 7]             10,759\n",
    "\n",
    "we will remove the last layer only(the classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedInceptionResNetV2(nn.Module):\n",
    "    \"\"\"ModifiedInceptionResNetV2 class for transfer learning with custom classifier.\n",
    "\n",
    "      This class implements a modified version of the Inception ResNet V2 model for image classification tasks. \n",
    "      It leverages transfer learning by freezing the pre-trained feature extraction layers from a \n",
    "      provided Inception ResNet V2 model and adding a custom classifier on top.\n",
    "\n",
    "      Args:\n",
    "          original_model (torchvision.models.InceptionV3): A pre-trained Inception ResNet V2 model \n",
    "              (typically loaded with `pretrained=True`).\n",
    "          num_classes (int, optional): The number of output classes for the classification task. \n",
    "              Defaults to 7.\n",
    "\n",
    "      Attributes:\n",
    "          features (nn.Sequential): A sequential container holding all layers from the original model \n",
    "              except the final classifier (Softmax layer).\n",
    "          classifier (nn.Sequential): A custom classifier consisting of:\n",
    "              - nn.Flatten(): Flattens the input from the feature extractor.\n",
    "              - nn.Linear(1536, 64): First fully-connected layer with 64 units and ReLU activation.\n",
    "              - nn.Linear(64, num_classes): Second fully-connected layer with 'num_classes' units \n",
    "                and Softmax activation for probability distribution of the classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_model, num_classes=7):\n",
    "        super(ModifiedInceptionResNetV2, self).__init__()\n",
    "\n",
    "        # Retain all layers except the final classifier(Softmax)\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "\n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # 1536 output from the last layer after removing the classifier\n",
    "            nn.Linear(1536, 64),  # First fully connected layer \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),  # Second fully connected layer\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_model = ModifiedInceptionResNetV2(model, num_classes=7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modified_model.to(device)\n",
    "# Inspect the modified model\n",
    "summary(modified_model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Training**\n",
    "\n",
    "*The problem we counter while reproducing the paper that isnâ€™t specified how much they augmented the Train data.*\n",
    "\n",
    "> In this work we used a stochastic gradient descent optimizer (SGD), with learning rate set to 0.0006 and usage of momentum and Nesterov Accelerated Gradient in order to adapt updates to the slope of the loss function (categorical cross entropy) and speed up the training process.\n",
    "\n",
    "> The total number of epochs was set to 100, using a small batch size of 10. A maximum patience of 15 epochs was set to the early stopping callback in order to mitigate the overfitting.\n",
    "\n",
    "> In order to improve classification performance, specially on minority classes, we loaded the best model obtained in the first round to extend the training phase and explore other potential local minimum points of the loss function, by using an additional amount of 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modified_model.parameters(), lr=0.0006,momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_loader, val_loader, model, optimizer, criterion, patience, early_stopping=True):\n",
    "    \"\"\"Trains a deep learning model for image classification with early stopping.\n",
    "\n",
    "    This function trains a provided model (`model`) on a given dataset (`train_loader`) \n",
    "    for a specified number of epochs (`num_epochs`). It also performs validation \n",
    "    on a separate dataset (`val_loader`) to monitor performance and potentially apply \n",
    "    early stopping to prevent overfitting.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): The number of training epochs.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for training data.\n",
    "        val_loader (torch.utils.data.DataLoader): The data loader for validation data.\n",
    "        model (torch.nn.Module): The deep learning model to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating model weights.\n",
    "        criterion (torch.nn.Module): The loss function used for calculating training loss.\n",
    "        patience (int): The number of epochs to wait for improvement in validation loss \n",
    "            before triggering early stopping (if enabled).\n",
    "        early_stopping (bool, optional): A flag to enable early stopping (default: True).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained model with the best weights found during validation.\n",
    "    \"\"\"\n",
    "    # Track best validation loss and patience counter for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Move the model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize running loss for the epoch\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # Transfer images and labels to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Clear gradients from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: predict on the images\n",
    "            outputs = model(images)\n",
    "            # Calculate the loss based on predictions and labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backpropagate the loss to update model weights\n",
    "            loss.backward()\n",
    "            # Update model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            # Accumulate the training loss for the epoch\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    model_name = f'{number}'\n",
    "    experiment_dir = '../experiments'\n",
    "    model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "    torch.save({\n",
    "        'model': model.state_dict()\n",
    "    }, model_directory)\n",
    "\n",
    "    print(f\"Model saved to checkpoint: {model_directory} as f'{model_name}.pt\")\n",
    "    print(\"Training and validation completed.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 500\n",
    "num_epochs = 100\n",
    "patience = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset, val_dataset, train_loader, val_loader = process_train_val_loader(number, train_transform, val_transform)\n",
    "print(\"Size of trainset : \" + str(len(train_dataset.image_list)))\n",
    "print(\"Size of validationset : \" + str(len(val_dataset.image_list)))\n",
    "trained_model = train(num_epochs, train_loader, val_loader, modified_model, optimizer, criterion, patience, early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(model, data_loader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            # Move images and labels to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get the index of the highest probability class\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and labels for the validation dataset\n",
    "val_preds, val_labels = get_predictions_and_labels(trained_model, val_loader, device)\n",
    "\n",
    "# Create a mapping from class names to indices\n",
    "label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
    "\n",
    "# Get class names in the correct order\n",
    "class_names = [class_name for class_name in class_files.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Comparing our results to paperâ€™s results**\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "</td>\n",
    "<td>\n",
    "Original results\n",
    "</td>\n",
    "<td>\n",
    "Our results\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Accuracy\n",
    "</td>\n",
    "<td>\n",
    "78.9%\n",
    "</td>\n",
    "<td>\n",
    "78.6%\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Number of epochs\n",
    "</td>\n",
    "<td>\n",
    "Approx. 42 epochs\n",
    "</td>\n",
    "<td>\n",
    "40 epochs\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Training size\n",
    "</td>\n",
    "<td>\n",
    "Unknown\n",
    "</td>\n",
    "<td>\n",
    "7000 samples\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Validation size\n",
    "</td>\n",
    "<td>\n",
    "478 samples\n",
    "</td>\n",
    "<td>\n",
    "478 samples\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Confusion martix\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/paper's_results.jpeg\" />\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Our_results.jpeg\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*for this experiment you can download the trained model parameters and the data used from https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp3/*"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
