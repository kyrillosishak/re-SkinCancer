{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyrillosishak/re-SkinCancer/blob/main/notebooks/exploreDuplicate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065b20de-8b12-43a2-a7a6-4dc466743052"
      },
      "source": [
        "# Toy Example"
      ],
      "id": "065b20de-8b12-43a2-a7a6-4dc466743052"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fde2d8f-227c-42a2-a7d3-c68d428dfd56"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Machine learning pipelines, like any code, are susceptible to errors. One such error, data leakage between training and testing data, can inflate a model‚Äôs apparent accuracy during evaluation. This can lead to deploying poor-performing models in production. Leakage often occurs unintentionally due to poor practices, and detecting it manually can be difficult. While encountering duplicate data in training and testing sets might seem like an obvious oversight, it‚Äôs surprisingly common \\[1\\].\n",
        "\n",
        "‚ú® The objective of this notebook is :\n",
        "\n",
        "-   Understand the concept of data leakage, specifically focusing on duplicate data leakage.\n",
        "-   Create synthetic data with intentional duplicates to illustrate the concept.\n",
        "-   Detect and mitigate duplicate data leakage in a real-world dataset (CIFAR-100).\n",
        "-   Train machine learning models and evaluate their performance before and after removing duplicates.\n",
        "-   Critically analyze the impact of duplicate data leakage on model performance.\n",
        "\n",
        "üîç In this notebook, we will explore: 1. A toy example with **synthetic data** to illustrate duplicate data leakage. 2. A real-world example using the **CIFAR-100** dataset.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Duplicates :** Duplicates in a dataset refer to instances that are either exactly the same or very similar to each other. They can arise due to various reasons during data collection and preprocessing. Duplicates can lead to overly optimistic evaluations of a machine learning model‚Äôs performance. This happens because the model might end up training on and testing against highly similar or identical instances, giving a false sense of its generalization capability.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/3kindOfDuplicates.png\" alt=\"duplicates\">\n",
        "</figure>\n",
        "\n",
        "This image from \\[2\\].\n",
        "\n",
        "*In the context of image data, the paper ‚ÄúDo we train on test data? Purging CIFAR of near-duplicates‚Äù \\[2\\] describes three types of duplication:*\n",
        "\n",
        "> -   **Exact Duplicate** Almost all pixels in the two images are approximately identical.\n",
        "> -   **Near-Duplicate** The content of the images is exactly the same, i.e., both originated from the same camera shot. However, different post-processing might have been applied to this original scene, e.g., color shifts, translations, scaling etc.\n",
        "> -   **Very Similar** The contents of the two images are different, but highly similar, so that the difference can only be spotted at the second glance.\n",
        "\n",
        "*In many situations, the distinction between `near-duplicate` and `very similar` isn‚Äôt crucial. **Near-duplicate** often serves as a more general term encompassing a wide range of image similarities. This category can include images derived from the same source with minor edits, but also extends to pictures of the same scene or object captured from different angles, cameras, or even screenshots.*\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**How Duplicate Samples Might End Up in Data?**"
      ],
      "id": "4fde2d8f-227c-42a2-a7d3-c68d428dfd56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56e75ad2-eb4f-483b-b397-bb71ac8237c0"
      },
      "source": [
        "**1. Reasons specific to the data and how it was collected** :\n",
        "\n",
        "-   *Scenario 1* : When training an email classifier for an academic department. In this scenario, data is collected from all students within the department. Since these students are part of the same academic environment, they receive a significant number of common emails, such as departmental announcements, course notifications, and event reminders.\n",
        "\n",
        "-   *Scenario 2* : When training a chatbot for customer support, data is often gathered from various interactions with customers. Many customers might ask similar questions or encounter the same issues, resulting in repeated dialogue patterns within the dataset.\n",
        "\n",
        "-   *Scenario 3* : When training a model to classify news articles, data might be sourced from various news outlets. Major news stories are often covered by multiple sources, and syndicated articles or press releases can appear across different outlets, leading to duplicate samples.\n",
        "\n",
        "-   *Scenario 4* : Speech recognition datasets often include recordings of common phrases or sentences. If data is collected from multiple participants who are asked to repeat specific phrases, duplicates are inevitable.\n",
        "\n",
        "-   *Scenario 5* : Frankenstein datasets (A ‚ÄúFrankenstein dataset‚Äù refers to a dataset composed of multiple other public datasets. Some scientists intentionally create Frankenstein datasets to augment training data, as it enhances model robustness by exposing it to diverse examples from multiple sources. This approach aims to mitigate bias and improve generalization, especially when individual datasets may be limited in scope or quality. However, unintentionally, duplicates can arise within these datasets when one dataset includes information that duplicates or overlaps with data already included from another source.)\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/covidx.png\" />\n",
        "\n",
        "    *This image from survey \\[3\\].*\n",
        "\n",
        "    *For example, consider the COVIDx dataset, which incorporates three main datasets: COHEN, RSNA, and CHOWDHURY. CHOWDHURY itself includes the COHEN dataset, this overlap might cause duplicates within the Frankenstein dataset \\[3\\].*"
      ],
      "id": "56e75ad2-eb4f-483b-b397-bb71ac8237c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5bd771-79f4-4b51-a85a-3bb1b4b9f613"
      },
      "source": [
        "**2. Using LLM to generate data for training** :\n",
        "\n",
        "Duplicates can arise in datasets created using LLMs due to several factors. First, limited prompt variety can lead the model to generate similar or identical outputs, especially if the same prompt is used repeatedly. Additionally, the training data of the LLM may contain repetitive patterns, causing the model to reproduce these during generation. Randomness in the text generation process, particularly with low diversity settings, can also result in repetitive sequences. Furthermore, using in-domain unlabeled examples or few-shot examples that are not diverse enough may limit the variability of the generated samples. To mitigate duplicates, it is essential to employ strategies such as diverse prompt design, careful sampling, and post-generation deduplication techniques.\n",
        "\n",
        "*This example shows how using LLM for generating dataset for specific task (sentiment analysis in our case) can cause duplicates:*"
      ],
      "id": "8d5bd771-79f4-4b51-a85a-3bb1b4b9f613"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d8b651f-8561-46bc-864c-98a5857ccc50"
      },
      "outputs": [],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install bitsandbytes accelerate xformers einops"
      ],
      "id": "4d8b651f-8561-46bc-864c-98a5857ccc50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7029fc62-401a-459f-b0fd-efc472c1dec1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import textwrap\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    prompt = f\"\"\"<s>[INST]{input_text}[/INST]\"\"\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    outputs = model.generate(**inputs,\n",
        "                             eos_token_id = 2,\n",
        "                             max_new_tokens=max_length,\n",
        "                             pad_token_id=1,\n",
        "                             temperature=0.1,\n",
        "                             do_sample=True)\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "    wrapped_text = wrap_text(text)\n",
        "    print(wrapped_text)\n",
        "    return wrapped_text"
      ],
      "id": "7029fc62-401a-459f-b0fd-efc472c1dec1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76d569d5-27a4-4857-b2b1-896442023f25"
      },
      "outputs": [],
      "source": [
        "torch.set_default_device('cuda')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", torch_dtype=\"auto\")"
      ],
      "id": "76d569d5-27a4-4857-b2b1-896442023f25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a38e0728-e7db-4cfb-a0c9-1d298ca1bc5b"
      },
      "outputs": [],
      "source": [
        "generate('Generate a sentences expressing positive sentiment:', max_length=512)"
      ],
      "id": "a38e0728-e7db-4cfb-a0c9-1d298ca1bc5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4036c19a-0a0d-460c-ac36-2d8f52a7113b"
      },
      "source": [
        "**3. Data augmentation or oversampling before split** :\n",
        "\n",
        "Data augmentation involves creating modified versions of existing data to increase the dataset size and diversity, while oversampling involves replicating data points to balance class distributions. If these techniques are applied before the dataset is split, the augmented or replicated samples can be distributed across the different splits. As a result, the same data point, or its augmented version, might appear in both the training and validation or test sets. This overlap can cause the model to have an unfair advantage, as it may encounter the same or very similar data during both training and evaluation phases. This can inflate performance metrics, giving a false sense of the model‚Äôs generalization capabilities.\n",
        "\n",
        "*In this example, we show how using augmentation before splitting can cause duplicate data leakage:*"
      ],
      "id": "4036c19a-0a0d-460c-ac36-2d8f52a7113b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca1b5373-3e5c-4c18-a850-f081e0a47dd7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True, num_workers=2)\n",
        "\n",
        "# Sample 100 images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Define augmentation\n",
        "augment = transforms.Compose([\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Convert to PIL images for augmentation\n",
        "images_pil = [transforms.ToPILImage()(img) for img in images]\n",
        "\n",
        "# Augment all of the images\n",
        "num_augment = len(images_pil)\n",
        "augmented_images = [(augment(img),idx) for idx, img in enumerate(images_pil[:num_augment])]\n",
        "normal_images =  [(transforms.ToTensor()(img),idx) for idx, img in enumerate(images_pil)]\n",
        "# Store indices with images\n",
        "augmented_images_with_indices =  augmented_images + normal_images\n",
        "train_images_with_indices, test_images_with_indices = train_test_split(augmented_images_with_indices, test_size=0.2, random_state=seed)\n",
        "\n",
        "# Find duplicates between train and test sets\n",
        "train_indices = set(idx for _, idx in train_images_with_indices)\n",
        "test_indices = set(idx for _, idx in test_images_with_indices)\n",
        "duplicates = train_indices.intersection(test_indices)\n",
        "\n",
        "print(f\"Duplicate indices between train and test sets: {duplicates}\")\n",
        "\n",
        "# Show duplicate images\n",
        "fig, axes = plt.subplots(len(duplicates), 2, figsize=(10, 5 * len(duplicates)))\n",
        "for i, idx in enumerate(duplicates):\n",
        "    train_img = next(img for img, id_ in train_images_with_indices if id_ == idx)\n",
        "    test_img = next(img for img, id_ in test_images_with_indices if id_ == idx)\n",
        "    axes[i, 0].imshow(np.transpose(train_img.numpy(), (1, 2, 0)))\n",
        "    axes[i, 0].set_title(f'Train Image Index: {idx}')\n",
        "    axes[i, 0].axis('off')\n",
        "    axes[i, 1].imshow(np.transpose(test_img.numpy(), (1, 2, 0)))\n",
        "    axes[i, 1].set_title(f'Test Image Index: {idx}')\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "ca1b5373-3e5c-4c18-a850-f081e0a47dd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7af0711b-8652-48d0-959f-c9db8cea23e7"
      },
      "source": [
        "## A toy example with synthetic data to illustrate duplicate data leakage."
      ],
      "id": "7af0711b-8652-48d0-959f-c9db8cea23e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17669c7-6899-4f8c-ade6-2b2b75d23c0c"
      },
      "source": [
        "### Example with accidental overlap between training and test set\n",
        "\n",
        "Illustration of the example: Here we demonstrate wrong data preprocessing that causes duplicate data leakage, which can bias the evaluation of our model."
      ],
      "id": "c17669c7-6899-4f8c-ade6-2b2b75d23c0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5262a8b5-c026-4f56-911d-4d2c540a9254"
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "5262a8b5-c026-4f56-911d-4d2c540a9254"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40e547df-3ad7-44ef-a9fb-63673798c8fb"
      },
      "outputs": [],
      "source": [
        "def generate_data(n_samples=100, n_features=1, noise_level=0.2):\n",
        "    \"\"\"\n",
        "    Generates synthetic data and casts it to the known distribution.\n",
        "    \"\"\"\n",
        "    # Generate random data\n",
        "    X = np.random.rand(n_samples, n_features)\n",
        "    # Known coefficients\n",
        "    coef = np.random.randn(n_features)\n",
        "    # Generate target variable\n",
        "    y = X @ coef + np.random.randn(n_samples) * noise_level\n",
        "    return X, y, coef"
      ],
      "id": "40e547df-3ad7-44ef-a9fb-63673798c8fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c87a1c4-d0f1-4722-83b9-b1afa4661948"
      },
      "outputs": [],
      "source": [
        "# Generate data with noise\n",
        "n_samples_initial = 100\n",
        "n_samples_total = 150\n",
        "noise_level = 0.3  # Change this value to experiment with different noise levels\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate initial data\n",
        "X, y, coef = generate_data(n_samples=n_samples_initial, noise_level=noise_level)\n",
        "\n",
        "# Sample with replacement\n",
        "indices = np.random.choice(np.arange(len(X)), size=n_samples_total, replace=True)\n",
        "X_sampled, y_sampled, indices = X[indices], y[indices], indices\n",
        "num_duplicates = len(indices) - len(np.unique(indices))\n",
        "\n",
        "# Add noise to features\n",
        "X_noisy = X_sampled + np.random.randn(*X_sampled.shape) * noise_level\n",
        "\n",
        "# Split into training and test sets according to the indices\n",
        "# Use 80% for training and 20% for testing\n",
        "train_size = int(0.8 * n_samples)\n",
        "train_indices = indices[:train_size]\n",
        "test_indices = indices[train_size:]\n",
        "\n",
        "X_train, y_train = X_noisy[:train_size], y_sampled[:train_size]\n",
        "X_test, y_test = X_noisy[train_size:], y_sampled[train_size:]\n",
        "\n",
        "# Calculate number of duplicates in test set that are also in train set\n",
        "duplicates_in_test = len(set(train_indices) & set(test_indices))\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on \"bad\" test set\n",
        "y_pred = model.predict(X_test)\n",
        "mse_bad_test = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Generate a new \"clean\" test set (for comparison)\n",
        "X_clean_test, y_clean_test, _ = generate_data(n_samples=len(y_test), noise_level=noise_level)\n",
        "\n",
        "# Evaluate on the \"clean\" test set\n",
        "y_pred_clean = model.predict(X_clean_test)\n",
        "mse_clean_test = mean_squared_error(y_clean_test, y_pred_clean)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of duplicates in sampled data: {num_duplicates}\")\n",
        "print(f\"Number of duplicates in test set that are also in train set: {duplicates_in_test}\")\n",
        "print(f\"MSE on 'bad' test set: {mse_bad_test}\")\n",
        "print(f\"MSE on 'clean' test set: {mse_clean_test}\")"
      ],
      "id": "5c87a1c4-d0f1-4722-83b9-b1afa4661948"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c70304-4ed0-4820-8ae9-9e74fb1ebefc"
      },
      "source": [
        "#### ü§î Why MSE on `bad` test set is lower than MSE on `clean` test set?"
      ],
      "id": "69c70304-4ed0-4820-8ae9-9e74fb1ebefc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0587b2e3-e216-4377-8bdd-d10cafe11868"
      },
      "source": [
        "Because the ‚Äòbad‚Äô test set has data leakage which can cause overly optimistic results.\n",
        "\n",
        "*Note : sometimes the MSE on `bad` test data perform worse (higher MSE) than `clean` test data that can be as both the noisy and clean datasets are generated with some randomness, leading to variability in the MSE. Even though the clean set doesn‚Äôt have duplicates, the particular noise added could make it slightly easier or harder to predict accurately compared to the noisy dataset.*"
      ],
      "id": "0587b2e3-e216-4377-8bdd-d10cafe11868"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4995bd0-ddcb-418a-a545-4ec82ef7b09e"
      },
      "source": [
        "### Example with incorrect oversampling"
      ],
      "id": "e4995bd0-ddcb-418a-a545-4ec82ef7b09e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40798e5d-e493-4c1b-9f00-85078e47979d"
      },
      "source": [
        "**Example :**"
      ],
      "id": "40798e5d-e493-4c1b-9f00-85078e47979d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed92ddbc-4845-4d8d-89fc-2037d96f5012"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Add noise to the data\n",
        "noise = np.random.normal(0, 1, X.shape)\n",
        "X_noisy = X + noise\n",
        "\n",
        "# Convert to DataFrame for better visualization (optional)\n",
        "df = pd.DataFrame(X_noisy)\n",
        "df['target'] = y"
      ],
      "id": "ed92ddbc-4845-4d8d-89fc-2037d96f5012"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "349403f1-a9bc-43f0-aa33-818a778b3ce9"
      },
      "outputs": [],
      "source": [
        "# Incorrect implementation with data leakage\n",
        "X_new, y_new = SMOTE().fit_resample(X_noisy, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_incorrect = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
        "predictions_incorrect = rf_incorrect.predict(X_test)\n",
        "\n",
        "accuracy_incorrect = accuracy_score(y_test, predictions_incorrect)\n",
        "print(f\"Incorrect Implementation Accuracy: {accuracy_incorrect:.4f}\")"
      ],
      "id": "349403f1-a9bc-43f0-aa33-818a778b3ce9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c77a6ed-0d67-4db4-91a4-9dcc47cf1f8a"
      },
      "source": [
        "##### ü§î Think why this is a bad implementation?"
      ],
      "id": "6c77a6ed-0d67-4db4-91a4-9dcc47cf1f8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6321991a-2582-401f-b32b-e4be8f9af0fc"
      },
      "source": [
        "Oversampling with SMOTE is performed **before** the train/test split. This creates a problem because the synthetic data generated by SMOTE are based on the original dataset‚Äôs samples. When you split the data afterward, there is a high chance that the synthetic data in the training set will have very similar counterparts (or even the same ones) in the test set. This causes data leakage, where information from the training set influences the test set, leading to overly optimistic performance estimates."
      ],
      "id": "6321991a-2582-401f-b32b-e4be8f9af0fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799c6c3e-316f-4856-9676-1b3371565cf7"
      },
      "source": [
        "#### üí° Solution :"
      ],
      "id": "799c6c3e-316f-4856-9676-1b3371565cf7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "944774c1-ffe7-48c5-b556-fc53612fb286"
      },
      "source": [
        "To avoid this issue, you should perform the train/test split before applying SMOTE. This ensures that the synthetic data generation only affects the training set, keeping the test set independent and truly representative of unseen data.\n",
        "\n",
        "We can edit the code to :"
      ],
      "id": "944774c1-ffe7-48c5-b556-fc53612fb286"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0c7f0fe-0d18-4756-ad82-800c78edb19c"
      },
      "outputs": [],
      "source": [
        "# Correct implementation without data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)\n",
        "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n",
        "\n",
        "rf_correct = RandomForestClassifier(random_state=42).fit(X_train_resampled, y_train_resampled)\n",
        "predictions_correct = rf_correct.predict(X_test)\n",
        "\n",
        "accuracy_correct = accuracy_score(y_test, predictions_correct)\n",
        "print(f\"Correct Implementation Accuracy: {accuracy_correct:.4f}\")"
      ],
      "id": "f0c7f0fe-0d18-4756-ad82-800c78edb19c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ecc9e8-a862-49df-a354-709e13634f9a"
      },
      "source": [
        "### Exercise"
      ],
      "id": "22ecc9e8-a862-49df-a354-709e13634f9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3245ab9d-745a-4322-84fc-2d297ffe2fdb"
      },
      "source": [
        "Identify the data leakage that caused by duplicates\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "from sklearn . feature_selection import SelectPercentile, chi2\n",
        "from sklearn . model_selection import LinearRegression ,Ridge\n",
        "\n",
        "X_0 , y = load_data ()\n",
        "\n",
        "select = SelectPercentile( chi2 , percentile =50)\n",
        "select.fit(X_0)\n",
        "X = select.transform( X_0 )\n",
        "\n",
        "X_train , y_train , X_test , y_test = train_test_split (X ,y)\n",
        "lr = LinearRegression ()\n",
        "lr.fit ( X_train , y_train )\n",
        "lr_score = lr.score ( X_test , y_test )\n",
        "\n",
        "ridge = Ridge ()\n",
        "ridge.fit (X , y)\n",
        "ridge_score = ridge.score (X_test , y_test)\n",
        "final_model = lr if lr_score > ridge_score else ridge\n",
        "```"
      ],
      "id": "3245ab9d-745a-4322-84fc-2d297ffe2fdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a46310a8-b00c-47b1-aa99-9dfc9018d556"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Correct the data leakage that caused by duplicates"
      ],
      "id": "a46310a8-b00c-47b1-aa99-9dfc9018d556"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4da516e-1940-4d05-a10d-865233737ddb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn . feature_selection import SelectPercentile, chi2\n",
        "from sklearn . model_selection import LinearRegression ,Ridge\n",
        "\n",
        "X_0 , y = load_data ()\n",
        "\n",
        "## Write your code here ##\n",
        "\n",
        "# Use the same approach as above\n",
        "\n",
        "#########################\n",
        "lr = LinearRegression ()\n",
        "lr.fit ( X_train , y_train )\n",
        "lr_score = lr.score ( X_test , y_test )\n",
        "\n",
        "ridge = Ridge ()\n",
        "ridge.fit (X , y)\n",
        "ridge_score = ridge.score (X_test , y_test)\n",
        "final_model = lr if lr_score > ridge_score else ridge"
      ],
      "id": "b4da516e-1940-4d05-a10d-865233737ddb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dce36b8-8fa9-44ad-96f0-17e852355e61"
      },
      "source": [
        "## A real-world example using the CIFAR-100 dataset"
      ],
      "id": "9dce36b8-8fa9-44ad-96f0-17e852355e61"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5437a1f-c027-466f-b185-bf83ed81d647"
      },
      "source": [
        "The CIFAR-100 dataset consists of 60,000 32x32 color images in 100 classes, with 600 images per class. There are 50,000 training images and 10,000 test images. It has been discovered \\[2\\] that CIFAR-100 dataset has 3 kinds of duplicates We will use CIFAR-100 to demonstrate how duplicates in the dataset can lead to data leakage and affect model performance.\n",
        "\n",
        "*We will use PyTorch for this tutorial. Not familiar with PyTorch [check](https://www.tutorialspoint.com/pytorch/index.htm)*\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "a5437a1f-c027-466f-b185-bf83ed81d647"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcdb6c3d-a77c-4c26-98ad-a6dcca62c634"
      },
      "source": [
        "We will start by importing the required modules"
      ],
      "id": "dcdb6c3d-a77c-4c26-98ad-a6dcca62c634"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67f312aa-fb60-4fec-8540-0042b8765d13"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from torchvision.models import resnet18\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "67f312aa-fb60-4fec-8540-0042b8765d13"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642cc6df-8474-44cb-8221-ee5e0a607766"
      },
      "source": [
        "Then we will download the original `CIFAR-100` dataset from `torchvision.datasets` and download the `ciFAIR` dataset from the official github repo."
      ],
      "id": "642cc6df-8474-44cb-8221-ee5e0a607766"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0315641a-204c-4082-9f2a-be7698b727fd"
      },
      "outputs": [],
      "source": [
        "# We will create a class for the ciFAIR dataset\n",
        "class ciFAIR100(torchvision.datasets.CIFAR100):\n",
        "    base_folder = 'ciFAIR-100'\n",
        "    url = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-100.zip'\n",
        "    filename = 'ciFAIR-100.zip'\n",
        "    tgz_md5 = 'ddc236ab4b12eeb8b20b952614861a33'\n",
        "    test_list = [\n",
        "        ['test', '8130dae8d6fc6a436437f0ebdb801df1'],\n",
        "    ]\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074, 0.4867, 0.4411), (0.2011, 0.1987, 0.2025)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5074, 0.4867, 0.4411), (0.2011, 0.1987, 0.2025)),\n",
        "])\n",
        "\n",
        "\n",
        "# # Downloading CIFAR-100 dataset to the 'data' directory and loading it\n",
        "train_data = CIFAR100(download=True, root=\"./data\", transform=transform_train)\n",
        "test_data = CIFAR100(root=\"./data\", train=False, transform=transform_test)\n",
        "\n",
        "# # Downloading ciFAIR-100 dataset to the 'data_fixed' directory and loading it\n",
        "train_data_fixed = ciFAIR100(download=True, root=\"./data_fixed\", transform=transform_train)\n",
        "test_data_fixed = ciFAIR100(root=\"./data_fixed\", train=False, transform=transform_test)"
      ],
      "id": "0315641a-204c-4082-9f2a-be7698b727fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d59c6d1-f367-4940-b299-a5f832b2bb34"
      },
      "source": [
        "Exploring the duplicate images"
      ],
      "id": "8d59c6d1-f367-4940-b299-a5f832b2bb34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ef9cdfb-6223-47f6-a3dc-4317a2871677"
      },
      "outputs": [],
      "source": [
        "# Downloading duplicate image information from the ciFAIR repository\n",
        "!wget https://github.com/cvjena/cifair/raw/master/meta/duplicates_cifar100.csv\n",
        "!wget https://github.com/cvjena/cifair/raw/master/meta/duplicates_cifar100_test.csv\n",
        "\n",
        "# Reading the CSV file containing duplicate information for train/test splits and test split\n",
        "train_test_duplicates = pd.read_csv('duplicates_cifar100.csv')\n",
        "# Reading the CSV file containing duplicate information for test splits itself (test split have duplicates)\n",
        "test_duplicates = pd.read_csv('duplicates_cifar100_test.csv')"
      ],
      "id": "2ef9cdfb-6223-47f6-a3dc-4317a2871677"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fb7c3f1-4ce2-4b82-9572-9aa6382a903d"
      },
      "outputs": [],
      "source": [
        "# Number of duplicates in each class :\n",
        "label = {\n",
        "    0: 'Real-duplicates',\n",
        "    1: 'Near-duplicates',\n",
        "    2: 'very similar'\n",
        "}\n",
        "print(\"duplicates in train/test splits\\n\")\n",
        "labels, counts = np.unique(train_test_duplicates['Judgment'], return_counts=True)\n",
        "for l, count in zip(labels, counts):\n",
        "    print(f\"Label: {label[l]}, Count: {count}\")\n",
        "\n",
        "print(\"\\nduplicates in test split\\n\")\n",
        "labels, counts = np.unique(test_duplicates['Judgment'], return_counts=True)\n",
        "for l, count in zip(labels, counts):\n",
        "    print(f\"Label: {label[l]}, Count: {count}\")"
      ],
      "id": "0fb7c3f1-4ce2-4b82-9572-9aa6382a903d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd5e52e5-60f2-4973-9086-9a74721bd207"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "categories = ['Dup.', 'Near-Dup.', 'Similar']\n",
        "datasets = ['CIFAR-100', 'CIFAR-100', 'CIFAR-100']\n",
        "training_counts = [39, 582, 270]\n",
        "test_counts = [2, 72, 30]\n",
        "\n",
        "# Positions of the bars\n",
        "bar_width = 0.5\n",
        "r1 = np.arange(len(categories))\n",
        "\n",
        "# Create the figure and increase the figure size\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Create the bars\n",
        "plt.bar(r1, training_counts, color='#1f77b4', width=bar_width, label='duplicates in trainset appeared in testset')\n",
        "plt.bar(r1, test_counts, color='#ff7f0e', bottom=training_counts, width=bar_width, label='duplicates in testset itself')\n",
        "\n",
        "# Adding the text labels and titles\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.xticks(r1, categories)\n",
        "plt.legend()\n",
        "\n",
        "# Add custom x-axis labels\n",
        "positions = [(r1[0] + r1[2]) / 2]\n",
        "dataset_labels = ['CIFAR-100']\n",
        "for pos, label in zip(positions, dataset_labels):\n",
        "    plt.text(pos, -50, label, ha='center')\n",
        "\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Setting x-ticks positions and labels\n",
        "plt.xticks(r1, categories, rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "cd5e52e5-60f2-4973-9086-9a74721bd207"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "032941fd-4136-4aae-86b3-cc8750ca7e94"
      },
      "outputs": [],
      "source": [
        "# run train_test_duplicates and see the similarity between each pair that classified as duplicates"
      ],
      "id": "032941fd-4136-4aae-86b3-cc8750ca7e94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "136371a9-a357-43fc-87dc-9f464f713fbc"
      },
      "outputs": [],
      "source": [
        "# Choosing random samples from the duplicates for visualization\n",
        "l = random.sample(range(0, 890), 5)\n",
        "\n",
        "# plotting the images\n",
        "for x in l:\n",
        "  if train_test_duplicates['Judgment'][x] == 0:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[train_test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(train_data.data[train_test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Real Duplicates')\n",
        "    plt.show()\n",
        "  elif train_test_duplicates['Judgment'][x] == 1:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[train_test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(train_data.data[train_test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Near Duplicate')\n",
        "    plt.show()\n",
        "  elif train_test_duplicates['Judgment'][x] == 2:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[train_test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(train_data.data[train_test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Very similar')\n",
        "    plt.show()"
      ],
      "id": "136371a9-a357-43fc-87dc-9f464f713fbc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bb6bb37-a691-4764-b665-98749cb5b8a2"
      },
      "outputs": [],
      "source": [
        "# Choosing random samples from the duplicates for visualization\n",
        "l = random.sample(range(0, 103), 5)\n",
        "\n",
        "# plotting the images\n",
        "for x in l:\n",
        "  if test_duplicates['Judgment'][x] == 0:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(test_data.data[test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Real Duplicates')\n",
        "    plt.show()\n",
        "  elif test_duplicates['Judgment'][x] == 1:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(test_data.data[test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Near Duplicate')\n",
        "    plt.show()\n",
        "  elif test_duplicates['Judgment'][x] == 2:\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(test_data.data[test_duplicates['TestID'][x]])\n",
        "    axes[1].imshow(test_data.data[test_duplicates['TrainID'][x]])\n",
        "    fig.suptitle('Very similar')\n",
        "    plt.show()"
      ],
      "id": "0bb6bb37-a691-4764-b665-98749cb5b8a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a01212a-31f8-4a0f-8d00-144a954dba67"
      },
      "source": [
        "Then we will start to train a ResNet CNN model using pyTorch on the original CIFAR dataset and assess its performance on the test split of the original dataset and the ciFAIR dataset test splits\n",
        "\n",
        "Note : both CIFAR-100 and ciFAIR **have the same** train data split the only difference is in the test set"
      ],
      "id": "6a01212a-31f8-4a0f-8d00-144a954dba67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b009faf9-c918-4e8b-b68f-19b1fea6240b"
      },
      "source": [
        "Currently will use this hyperparameters"
      ],
      "id": "b009faf9-c918-4e8b-b68f-19b1fea6240b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839a0d65-0eb1-406f-aece-01663c29d9e5"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4"
      ],
      "id": "839a0d65-0eb1-406f-aece-01663c29d9e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4db01e71-4dc8-48cb-be26-78570977a1e7"
      },
      "source": [
        "First we will use ResNet model without any pre-training"
      ],
      "id": "4db01e71-4dc8-48cb-be26-78570977a1e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d57c555-975e-4069-8475-cdd82ae9068d"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet model and modify the final layer for CIFAR-100\n",
        "model = resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(512, 100)  # CIFAR-100 has 100 classes\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "# This helps in fine-tuning the learning rate as training progresses, often leading to better performance.\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "id": "8d57c555-975e-4069-8475-cdd82ae9068d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe474184-6d00-4eb2-b481-27d814e34319"
      },
      "source": [
        "Then we will define the train/test functions"
      ],
      "id": "fe474184-6d00-4eb2-b481-27d814e34319"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ba0319-b593-492c-b7b6-9e49ec89cdd2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training function\n",
        "def train(epoch, trainloader):\n",
        "    # Sets the model to training mode. IMP for layers (dropout and batchnorm) that behave differently in train/eval mode.\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        # Move the input data and labels to the specified device (CPU or GPU).\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Testing function\n",
        "def test(testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
      ],
      "id": "70ba0319-b593-492c-b7b6-9e49ec89cdd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eebff1a-3a4b-442a-a47b-4ecb5d9ecb80"
      },
      "source": [
        "Load `PyTorch` data loader"
      ],
      "id": "9eebff1a-3a4b-442a-a47b-4ecb5d9ecb80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b16b1e9-5d14-4ad1-8995-57dd3f2c1ba5"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "testloader_fixed = DataLoader(test_data_fixed, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "id": "3b16b1e9-5d14-4ad1-8995-57dd3f2c1ba5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "063238cd-5029-4d3d-a89e-17d2172f2c44"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train(epoch,trainloader)\n",
        "    scheduler.step()\n",
        "print('Finished Training')\n",
        "# Testing\n",
        "print('Testing with original CIFAR testset')\n",
        "test(testloader)\n",
        "print('Testing with ciFAIR testset')\n",
        "test(testloader_fixed)"
      ],
      "id": "063238cd-5029-4d3d-a89e-17d2172f2c44"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf20a294-28d3-4533-9412-f61db3b39073"
      },
      "source": [
        "#### ‚ùî Why do you think the test set of the original split performs better than the test set of the ciFAIR dataset?"
      ],
      "id": "bf20a294-28d3-4533-9412-f61db3b39073"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90e03a98-c85d-417f-90b8-045efc05c44f"
      },
      "source": [
        "The model was trained on images in the original test set, so it performs very well on images it has seen before. However, it performs poorly on the ciFAIR test set, which contains images it has not seen before."
      ],
      "id": "90e03a98-c85d-417f-90b8-045efc05c44f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef81d8f9-ce46-4ba3-98f4-9538db6e9eab"
      },
      "source": [
        "## How to measure duplicates?\n",
        "\n",
        "In this section, we will discuss how to find duplicates in a dataset based on the following types:\n",
        "\n",
        "1.  Images\n",
        "2.  Texts\n",
        "3.  General data\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### **1. How to mitigate duplicates in Images:**\n",
        "\n",
        "*Finding duplicate images can be challenging due to variations in size, format, and slight alterations. Here are some common methods:*\n",
        "\n",
        "-   **Hashing** : Compute hash values for images and compare them. Techniques like MD5, SHA-1. This techniques can‚Äôt detect Near-duplicate it can detect Exact-duplicates only. *Example :*\n",
        "\n",
        "    ``` python\n",
        "    import hashlib\n",
        "    def calculate_sha256_hash(image_path):\n",
        "        with open(image_path, \"rb\") as f:\n",
        "            image_data = f.read()\n",
        "            return hashlib.sha256(image_data).hexdigest()\n",
        "    ```\n",
        "\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/dog.jpg\" width=\"400\" height=\"400\"/>\n",
        "\n",
        "    </td>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/dog_cropped.jpg\" width=\"400\" height=\"400\" />\n",
        "\n",
        "    </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "    <td>\n",
        "\n",
        "    Hash : 795d4344cdc8fa578b86d30622a8d935e237bcff406c4e5a1d6d17b568c73bfa\n",
        "\n",
        "    </td>\n",
        "    <td>\n",
        "\n",
        "    Hash : 553f7d8da6736af2cc237f9abad4c1f35b2d26705e1b87527f04c62c48f018f4\n",
        "\n",
        "    </td>\n",
        "    </tr>\n",
        "    </table>\n",
        "\n",
        "    *As we can see hash cannot detect if the image is augmented (cropped in our case) but can detect exact duplicates.*\n",
        "\n",
        "-   **Pixel-by-pixel cosine similarity** : It is a method used to detect duplicate or near-duplicate images by comparing the pixel values of two images. This technique measures the cosine of the angle between two vectors, which in this context are the pixel value arrays of the images. A threshold value can be set to determine if the images are considered duplicates. For example, a threshold of 0.95 might be used to account for minor variations while still recognizing duplicates.\n",
        "\n",
        "    ``` python\n",
        "      def cosine_similarity(vec1, vec2):\n",
        "          dot_product = np.dot(vec1, vec2)\n",
        "          magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
        "          if not magnitude:\n",
        "              return 0\n",
        "          return dot_product / magnitude\n",
        "\n",
        "      similarity = cosine_similarity(img1.flatten(), img2.flatten())\n",
        "    ```\n",
        "\n",
        "-   **Similarities of image embeddings** : Using image embeddings to detect duplicates involves leveraging deep learning models to extract high-level features from images, converting them into dense vectors (embeddings). First, a pre-trained convolutional neural network (CNN) such as VGG, ResNet, or Inception is used to generate embeddings for each image. These models, trained on large datasets. Next, the cosine similarity or Euclidean distance between the embeddings of different images is calculated. Cosine similarity measures the cosine of the angle between two vectors, indicating their similarity. Images with high cosine similarity are considered duplicates or near-duplicates. *Example showing comparing cosine similarities:*\n",
        "\n",
        "    ``` python\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torchvision.transforms as transforms\n",
        "    import torchvision.models as models\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Load pre-trained ResNet50 model\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model = nn.Sequential(*list(model.children())[:-1])  # Remove the last fully connected layer\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Paths to the images (adjust these paths as per your directory structure)\n",
        "    img1_path = \"dog.jpg\"\n",
        "    img2_path = \"dog_cropped.jpg\"\n",
        "    img3_path = \"another_dog.jpg\"\n",
        "\n",
        "    image1 = Image.open(img1_path).convert('RGB')\n",
        "    image2 = Image.open(img2_path).convert('RGB')\n",
        "    image3 = Image.open(img3_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess images\n",
        "    image1_tensor = preprocess(image1).unsqueeze(0)\n",
        "    image2_tensor = preprocess(image2).unsqueeze(0)\n",
        "    image3_tensor = preprocess(image3).unsqueeze(0)\n",
        "\n",
        "    # Compute image embeddings using ResNet50\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embedding1 = model(image1_tensor).squeeze().cpu().numpy()\n",
        "        embedding2 = model(image2_tensor).squeeze().cpu().numpy()\n",
        "        embedding3 = model(image3_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarity_dog_augmented = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0][0]\n",
        "    similarity_unrelated_dog = cosine_similarity(embedding1.reshape(1, -1), embedding3.reshape(1, -1))[0][0]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Similarity between original dog and augmented dog: {similarity_dog_augmented:.4f}\")\n",
        "    print(f\"Similarity between original dog and unrelated dog: {similarity_unrelated_dog:.4f}\")\n",
        "    ```\n",
        "\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/dog.jpg\" width=\"400\" height=\"400\"/>\n",
        "\n",
        "    </td>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/dog_cropped.jpg\" width=\"400\" height=\"400\" />\n",
        "\n",
        "    </td>\n",
        "    </tr>\n",
        "    </table>\n",
        "\n",
        "    *Cosine similarity between 2 images : 0.8782*\n",
        "\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/dog.jpg\" width=\"400\" height=\"400\"/>\n",
        "\n",
        "    </td>\n",
        "    <td>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/another_dog.jpg\" width=\"400\" height=\"400\" />\n",
        "\n",
        "    </td>\n",
        "    </tr>\n",
        "    </table>\n",
        "\n",
        "    *Cosine similarity between 2 images : 0.4228*\n",
        "\n",
        "    *As observed, the cosine similarity between the original dog photo and the cropped dog photo is significantly higher compared to the similarity between the original dog photo and the unrelated dog photo. This method enables us to effectively identify duplicates within a dataset. This images from \\[5\\].*\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### **2. How to mitigate duplicates in Texts:**\n",
        "\n",
        "*Duplicate detection in text data involves identifying identical or nearly identical pieces of text. In the context of image data, the paper ‚ÄúDeduplicating Training Data Makes Language Models Better‚Äù \\[4\\] describes two ways of detecting duplicates:*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/textduplicates.png\">\n",
        "\n",
        "-   **Exact Substring Duplication** : Due to the diversity of human language, it is uncommon for the same idea to be expressed identically in multiple documents unless one is derived from the other or both quote a shared source. When two text examples share a sufficiently long substring. Based on statistical analyses, a minimum matching substring length of 50 tokens is selected.\n",
        "    -   **Suffix Arrays**: Exact substring matching is computationally prohibitive with naive (quadratic) all-pair matching. To improve efficiency, all examples in the dataset are concatenated into a giant sequence, from which a Suffix Array is constructed. A suffix array is a representation of a suffix tree that can be constructed in linear time and allows efficient computation of many substring queries.\n",
        "\n",
        "        *For example, the suffixes of the sequence ‚Äúbanana‚Äù are (‚Äúbanana‚Äù, ‚Äúanana‚Äù, ‚Äúnana‚Äù ‚Äúana‚Äù, ‚Äúna‚Äù, ‚Äúa‚Äù) and so the suffix array is the sequence (6 4 2 1 5 3).*\n",
        "\n",
        "    -   **Substring matching**: Identify Duplicates by scanning the suffix array, repeated sequences can be identified as adjacent indices in the array. If two sequences share a common prefix of at least the threshold length, they are recorded as duplicates.\n",
        "\n",
        "*This example illustrate the first approach :*"
      ],
      "id": "ef81d8f9-ce46-4ba3-98f4-9538db6e9eab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e721905d-2709-4fa6-a24e-b8a56bdff0f2"
      },
      "outputs": [],
      "source": [
        "# Input text\n",
        "text = \"the quick brown fox jumps over the lazy dog jumps over the quick brown fox.\"\n",
        "\n",
        "# Step 1: Build suffix array\n",
        "suffixes = [(text[i:], i) for i in range(len(text))]  # Create list of suffixes with their indices\n",
        "suffixes.sort()  # Sort the suffixes lexicographically\n",
        "suffix_array = [suffix[1] for suffix in suffixes]  # Extract indices from sorted suffixes\n",
        "print(\"Suffix Array:\", suffix_array)\n",
        "\n",
        "# Step 2: Find duplicates using the suffix array\n",
        "min_length = 7\n",
        "duplicates = []\n",
        "n = len(suffix_array)\n",
        "\n",
        "for i in range(n - 1):\n",
        "    lcp_length = 0  # Initialize the length of the longest common prefix (LCP)\n",
        "\n",
        "    # Calculate the LCP between consecutive suffixes\n",
        "    while (suffix_array[i] + lcp_length < len(text) and\n",
        "           suffix_array[i+1] + lcp_length < len(text) and\n",
        "           text[suffix_array[i] + lcp_length] == text[suffix_array[i+1] + lcp_length]):\n",
        "        lcp_length += 1\n",
        "\n",
        "    # Check if the LCP length is greater than or equal to the minimum length\n",
        "    if lcp_length >= min_length:\n",
        "        duplicates.append((suffix_array[i], suffix_array[i+1], lcp_length))\n",
        "\n",
        "print(\"Duplicates:\", duplicates)\n",
        "\n",
        "# Step 3: Print duplicate substrings\n",
        "for start1, start2, length in duplicates:\n",
        "    duplicate_substring = text[start1:start1+length]\n",
        "    print(f\"Duplicate substring: '{duplicate_substring}' found at indices {start1} and {start2}\")"
      ],
      "id": "e721905d-2709-4fa6-a24e-b8a56bdff0f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90896f2b-4f39-4a30-b28b-245086683524"
      },
      "source": [
        "-   **Approximate Matching with MinHash** : Exact substring matching may not be sufficient for all cases, especially with web crawl text where documents might be identical except for minor variations. For such cases, approximate deduplication using MinHash is effective. MinHash is an approximate matching algorithm widely used in large-scale deduplication tasks. MinHash approximates the Jaccard Index, which measures the similarity between two sets of n-grams derived from documents. The algorithm uses hash functions to create document signatures by sorting n-grams and keeping only the smallest hashed n-grams.\n",
        "\n",
        "    -   Generate N-grams\n",
        "    -   Use hash functions to generate MinHash signatures by selecting the smallest hashed n-grams.\n",
        "    -   Calculate the probability that two documents are potential matches based on their MinHash signatures.\n",
        "\n",
        "*This example illustrate the second approach :*"
      ],
      "id": "90896f2b-4f39-4a30-b28b-245086683524"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb4ddaeb-fef7-4442-b82e-91c51c342368"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "# Input texts\n",
        "text1 = \"the quick brown fox\"\n",
        "text2 = \"the quick brown fox jumps over the lazy dog\"\n",
        "text3 = \"I love ML\"\n",
        "n = 3\n",
        "\n",
        "# Step 1: Generate n-grams\n",
        "ngrams1 = [text1[i:i+n] for i in range(len(text1)-n+1)]\n",
        "ngrams2 = [text2[i:i+n] for i in range(len(text2)-n+1)]\n",
        "ngrams3 = [text3[i:i+n] for i in range(len(text3)-n+1)]\n",
        "print(\"N-grams 1:\", ngrams1)\n",
        "print(\"N-grams 2:\", ngrams2)\n",
        "print(\"N-grams 3:\", ngrams3)\n",
        "\n",
        "# Step 2: Compute MinHash signatures\n",
        "num_hashes = 100\n",
        "signature1 = [min([int(hashlib.md5((str(seed) + ngram).encode()).hexdigest(), 16)\n",
        "                  for ngram in ngrams1])\n",
        "              for seed in range(num_hashes)]\n",
        "signature2 = [min([int(hashlib.md5((str(seed) + ngram).encode()).hexdigest(), 16)\n",
        "                  for ngram in ngrams2])\n",
        "              for seed in range(num_hashes)]\n",
        "signature3 = [min([int(hashlib.md5((str(seed) + ngram).encode()).hexdigest(), 16)\n",
        "                  for ngram in ngrams3])\n",
        "              for seed in range(num_hashes)]\n",
        "print(\"MinHash Signature 1:\", signature1)\n",
        "print(\"MinHash Signature 2:\", signature2)\n",
        "print(\"MinHash Signature 3:\", signature3)\n",
        "\n",
        "# Step 3: Calculate Jaccard and MinHash similarities\n",
        "jaccard_sim1_2 = len(set(ngrams1).intersection(set(ngrams2))) / len(set(ngrams1).union(set(ngrams2)))\n",
        "jaccard_sim1_3 = len(set(ngrams1).intersection(set(ngrams3))) / len(set(ngrams1).union(set(ngrams3)))\n",
        "minhash_sim1_2 = sum(1 for i in range(num_hashes) if signature1[i] == signature2[i]) / num_hashes\n",
        "minhash_sim1_3 = sum(1 for i in range(num_hashes) if signature1[i] == signature3[i]) / num_hashes\n",
        "print(\"Jaccard Similarity between text1 and text2:\", jaccard_sim1_2)\n",
        "print(\"Jaccard Similarity between text1 and text3:\", jaccard_sim1_3)\n",
        "print(\"MinHash Similarity between text1 and text2:\", minhash_sim1_2)\n",
        "print(\"MinHash Similarity between text1 and text3:\", minhash_sim1_3)"
      ],
      "id": "bb4ddaeb-fef7-4442-b82e-91c51c342368"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f12c47c-c463-4b85-89ce-b4544d1a13ad"
      },
      "source": [
        "*As observed in this example, text1 has similarities with text2 but has 0 similarity with text3.*\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### How to mitigate duplicates in General Data:\n",
        "\n",
        "*For general data, which can include structured data in tables, finding duplicates often involves comparing multiple columns.*\n",
        "\n",
        "-   Comparing entire records for exact matches.\n",
        "-   Clustering grouping similar records together based on features.\n",
        "-   Assigning probabilities to different fields and calculating an overall similarity score."
      ],
      "id": "9f12c47c-c463-4b85-89ce-b4544d1a13ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620c6e3d-57b1-4000-b334-45017aaacaa3"
      },
      "source": [
        "## References\n",
        "\n",
        "\\[1\\] [Leakage and the reproducibility crisis in machine learning-based science](https://arxiv.org/abs/2207.07048)\n",
        "\n",
        "\\[2\\] [Do we train on test data? Purging CIFAR of near-duplicates](https://arxiv.org/pdf/1902.00423)\n",
        "\n",
        "\\[3\\] [Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans](https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-021-00307-0/MediaObjects/42256_2021_307_MOESM1_ESM.pdf)\n",
        "\n",
        "\\[4\\] [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/pdf/2107.06499)\n",
        "\n",
        "\\[5\\] [National geographic](https://www.nationalgeographic.com/animals/mammals/facts/domestic-dog)"
      ],
      "id": "620c6e3d-57b1-4000-b334-45017aaacaa3"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  }
}