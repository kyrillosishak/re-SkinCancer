{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                                 |\n",
    "|-------------------------------------------------|\n",
    "| title: “Fixing data leakage issue in HAM_10000” |\n",
    "| author: “Kyrillos Ishak”                        |\n",
    "| date: “`r Sys.Date()`”                          |\n",
    "| output:                                         |\n",
    "| html_notebook:                                  |\n",
    "| pandoc_args: \\[                                 |\n",
    "| “–number-offset=1,0”                            |\n",
    "| \\]                                              |\n",
    "| fig_caption: true                               |\n",
    "| number_sections: yes                            |\n",
    "| toc: yes                                        |\n",
    "| toc_depth: 3                                    |"
   ],
   "id": "168d488d-e06d-40e2-89ca-03892448b741"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Data Leakage in Skin Cancer Classification with Transfer Learning\n",
    "\n",
    "`Skin Cancer Classification with Transfer Learning` paper that we are discussing uses HAM_10000 dataset.\n",
    "\n",
    "The HAM_10000 dataset is a widely used dataset for dermatological image classification, containing thousands of images of pigmented lesions. While it has been a valuable resource for training machine learning models, recent studies have highlighted significant issues with data leakage due to duplicate images within the dataset. This data leakage can lead to overly optimistic performance estimates and undermine the validity of research findings \\[2\\].\n",
    "\n",
    "In this notebook, we tackle the data leakage problem in the HAM_10000 dataset by addressing the duplicates within the dataset. Notably, insights from the survey paper “Leakage and the Reproducibility Crisis in ML-based Science” highlighted the issue of data leakage and its detrimental effects on reproducibility in machine learning. This survey also references the study “Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets,” which provided evidence of duplicate images in dermatological datasets, including `HAM_10000`\\[1\\].\n",
    "\n",
    "Our approach involves two main notebooks: the first reproduces the original paper, and the second focuses on identifying and addressing data leakage in the HAM_10000 dataset. By showing image similarity and identifying duplicates, we will clean the validation dataset and subsequently evaluate the impact on model accuracy and confusion metrics.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "**🔍 In this notebook, we will:**\n",
    "\n",
    "1.  Identify Duplicates in `HAM_10000`4\n",
    "2.  Clean the Validation Dataset.\n",
    "3.  Evaluate Model Performance on new clean validation data.\n",
    "4.  Discuss Implications."
   ],
   "id": "f80f691b-f1cf-475a-9e53-48aebc718976"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Identify Duplicates in HAM_10000**\n",
    "\n",
    "**`HAM_10000` have severe flaw :**\n",
    "\n",
    "> A caveat of HAM10000, despite its rather large size, is that it contains multiple images of the same lesion captured either from different viewing angles or at different magnification levels\n",
    "\n",
    "> the number of lesions with unique lesion IDs (HAM_xxx) is smaller than the number of images with unique image IDs (ISIC_xxx).\n",
    "\n",
    "> observe that the 10,015 images are in fact derived from only 7,470 unique lesions, and 1,956 of these lesion IDs (∼26.18%) contains 2 or more images: 1,423 lesions have 2 images, 490 lesions have 3 images, 34 lesions have 4 images, 5 lesions have 5 images, and 6 lesions have 4 images each.\n",
    "\n",
    "<img src = \"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Near-duplicate_HAM10000.png\" height = 150> <img src = \"https://raw.githubusercontent.com/kyrillosishak/re-SkinCancer/main/assets/Near-duplicate2_HAM10000.png\" height = 150>\n",
    "\n",
    "*This images from \\[2\\].*\n",
    "\n",
    "This results in near-duplicates within the dataset, which can compromise the integrity of any machine learning model trained on it. The presence of near-duplicate images can lead to data leakage, where the model learns to recognize specific lesions rather than generalizing to new, unseen lesions.\n",
    "\n",
    "*The duplicates were identified using a tool called [FastDup](https://github.com/visual-layer/fastdup) in the repository.*"
   ],
   "id": "e9f30a54-3a16-41f9-8aec-f59b7dccfd2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset to dir data\n",
    "!curl -L -O -J -H \"X-Dataverse-key:$API_TOKEN\" https://dataverse.harvard.edu/api/access/dataset/:persistentId/?persistentId=doi:10.7910/DVN/DBW86T\n",
    "!unzip -q dataverse_files.zip\n",
    "!mkdir data\n",
    "%cd data\n",
    "!unzip -q ../HAM10000_images_part_1.zip\n",
    "!unzip -q ../HAM10000_images_part_2.zip\n",
    "%cd .."
   ],
   "id": "b955130c-86df-409a-81e6-6e773e8f28a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file containing metadata for the HAM10000 dataset\n",
    "meta_data = pd.read_csv('HAM10000_metadata')\n",
    "\n",
    "# Group the image filenames by their class labels ('dx') and convert the groups to a dictionary\n",
    "# The dictionary keys are class labels and the values are lists of image filenames belonging to each class\n",
    "class_files = meta_data.groupby('dx')['image_id'].apply(list).to_dict()\n",
    "\n",
    "# Create a mapping from class names to integer indices\n",
    "# This is useful for converting class labels to numeric format for machine learning tasks\n",
    "label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
    "\n",
    "# Create a list of class names\n",
    "class_names = [class_name for class_name in class_files.keys()]\n",
    "\n",
    "print(\"Class names: \",class_names)\n",
    "print(\"Label map: \",label_map)"
   ],
   "id": "1f9f7d49-5ae4-4a21-8a3e-c00f50ed1097"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Clean the data**\n",
    "\n",
    "In `HAM_10000` there is problem in the data :\n",
    "\n",
    "It contains multiple images of the same lesion captured either from different viewing angles or at different magnification levels\n",
    "\n",
    "-   To fix this issue we will put the images with the same lesion_id in the train set and the rest of them will be in the validation set"
   ],
   "id": "960adaed-2ab9-4034-9fe0-bcd937a6a526"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "1cab19e2-2e99-4966-a537-329bae795f88"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ],
   "id": "6ca49eec-1d3c-4843-acfe-a4bc9f9cf248"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinLesionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading and transforming images of skin lesions along with their labels.\n",
    "\n",
    "    Args:\n",
    "        image_list (list of str): List of image filenames.\n",
    "        labels (list of int): List of labels corresponding to each image.\n",
    "        transform (callable, optional): Optional transform to be applied on an image sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_list, labels, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_list[idx]\n",
    "        img_path = \"./data/\"+img_path+\".jpg\"\n",
    "        image = PIL.Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ],
   "id": "6d88f5a6-2369-4a8c-a974-dfb734dd086d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_val_loader(df, target_size, train_transform, val_transform):\n",
    "    \"\"\"\n",
    "    Processes and prepares the training and validation data loaders.\n",
    "\n",
    "    Args:\n",
    "        target_size (int): The desired number of training images after augmentation.\n",
    "        train_transform (callable): Transformations to be applied to training images.\n",
    "        val_transform (callable): Transformations to be applied to validation images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training dataset, validation dataset, training data loader, and validation data loader.\n",
    "    \"\"\"\n",
    "    # Load metadata and initialize lists\n",
    "    class_files = df.groupby('dx')['image_id'].apply(list).to_dict()\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_files.keys())}\n",
    "    \n",
    "    train_images, train_labels, val_images, val_labels = [], [], [], []\n",
    "\n",
    "    # Process each class to remove confusing images and handle similar ones\n",
    "    for class_name, image_list in class_files.items():\n",
    "        \n",
    "        labels = [label_map[class_name]] * len(image_list)\n",
    "        train_dataset, val_dataset = augment_and_split_data(df, image_list, labels, train_transform, val_transform, target_size)\n",
    "        \n",
    "        train_images.extend(train_dataset.image_list)\n",
    "        train_labels.extend(train_dataset.labels)\n",
    "        val_images.extend(val_dataset.image_list)\n",
    "        val_labels.extend(val_dataset.labels)\n",
    "\n",
    "    train_dataset = SkinLesionDataset(train_images, train_labels, transform=train_transform)\n",
    "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    return train_dataset, val_dataset, train_loader, val_loader\n",
    "\n",
    "\n",
    "def augment_and_split_data(df, image_list, label_list, train_transform, val_transform, target_size=300):\n",
    "    \"\"\"\n",
    "    Augments and splits a dataset of images and labels into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'image_id' and 'lesion_id' columns.\n",
    "        image_list (list of str): List of image filenames.\n",
    "        label_list (list of int): List of labels corresponding to each image.\n",
    "        train_transform (callable): Transformations to be applied to training images.\n",
    "        val_transform (callable): Transformations to be applied to validation images.\n",
    "        target_size (int, optional): Desired number of training images after augmentation. Default is 300.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    combined = list(zip(image_list, label_list))\n",
    "    random.shuffle(combined)\n",
    "    image_list[:], label_list[:] = zip(*combined)\n",
    "\n",
    "    lesion_dict = df.set_index('image_id')['lesion_id'].to_dict()\n",
    "    lesion_groups = {lesion_id: [] for lesion_id in lesion_dict.values()}\n",
    "\n",
    "    for img, lbl in combined:\n",
    "        lesion_id = lesion_dict[img]\n",
    "        lesion_groups[lesion_id].append((img, lbl))\n",
    "\n",
    "    train_images_labels, val_images_labels = [], []\n",
    "\n",
    "    for lesion_id, imgs_labels in lesion_groups.items():\n",
    "        if len(val_images_labels) + len(imgs_labels) <= 0.2 * len(image_list) :\n",
    "            val_images_labels.extend(imgs_labels)\n",
    "        else:\n",
    "            train_images_labels.extend(imgs_labels)\n",
    "\n",
    "    train_images, train_labels = zip(*train_images_labels)\n",
    "    val_images, val_labels = zip(*val_images_labels)\n",
    "\n",
    "    if len(train_images) > 360:\n",
    "        train_images = train_images[:360]\n",
    "        train_labels = train_labels[:360]\n",
    "    if len(val_images) > 90:\n",
    "        val_images = val_images[:90]\n",
    "        val_labels = val_labels[:90]\n",
    "    \n",
    "    augmented_images, augmented_labels = [], []\n",
    "    while len(augmented_images) < target_size:\n",
    "        for img, label in zip(train_images, train_labels):\n",
    "            augmented_images.append(img)\n",
    "            augmented_labels.append(label)\n",
    "            if len(augmented_images) >= target_size:\n",
    "                break\n",
    "\n",
    "    train_dataset = SkinLesionDataset(augmented_images, augmented_labels, transform=train_transform)\n",
    "    val_dataset = SkinLesionDataset(val_images, val_labels, transform=val_transform)\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ],
   "id": "393a1fa9-7e2f-41b6-b0c6-23bd9901003c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=30,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=None,\n",
    "        shear=10\n",
    "    ),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "id": "ac91cb66-f991-4098-a870-fd4ef1f10831"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('HAM10000_metadata')"
   ],
   "id": "5d685f76-538c-4876-acaa-39f01cd138ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1000\n",
    "num_epochs = 100\n",
    "patience = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset, val_dataset, train_loader, val_loader = process_train_val_loader(meta_data, number, train_transform, val_transform)\n",
    "print(\"Size of trainset : \" + str(len(train_dataset.image_list)))\n",
    "print(\"Size of validationset : \" + str(len(val_dataset.image_list)))"
   ],
   "id": "3c3a0a3e-85c9-48f1-9526-fa8878d062ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data we should use the code in [Reproducing the original paper notebook](Markdowns/01-reproducingSkinCancer.md) to train the model on the clean data."
   ],
   "id": "a9090429-ee7b-406a-a3fd-c68e05a6f8a2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Evaluate Model Performance on new clean validation data**\n",
    "\n",
    "1.  Downloading the model trained on the cleaned data and the model trained on duplicated data.\n",
    "2.  Downloading the datasets that is used in every model.\n",
    "3.  Evaluate each model"
   ],
   "id": "73c9fc45-9c60-4693-b835-3f802569f0ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp4/val_loader_clean_lesion_only.pt\n",
    "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp2/val_loader.pt"
   ],
   "id": "6f94158c-67fb-4081-9767-62423403a1e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader_clean = torch.load('val_loader_clean_lesion_only.pt')\n",
    "val_loader = torch.load('val_loader.pt')"
   ],
   "id": "9276885e-22f9-46f2-8a62-9992097c0733"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp4/1000_clean_lesion_only.pt\n",
    "!wget -q https://huggingface.co/KyrillosIshak/Re-SkinCancer/resolve/main/Experiments/exp2/1000.pt"
   ],
   "id": "59bfb2a4-ba81-4072-966a-412477179719"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedInceptionResNetV2(nn.Module):\n",
    "    \"\"\"ModifiedInceptionResNetV2 class for transfer learning with custom classifier.\n",
    "\n",
    "      This class implements a modified version of the Inception ResNet V2 model for image classification tasks.\n",
    "      It leverages transfer learning by freezing the pre-trained feature extraction layers from a\n",
    "      provided Inception ResNet V2 model and adding a custom classifier on top.\n",
    "\n",
    "      Args:\n",
    "          original_model (torchvision.models.InceptionV3): A pre-trained Inception ResNet V2 model\n",
    "              (typically loaded with `pretrained=True`).\n",
    "          num_classes (int, optional): The number of output classes for the classification task.\n",
    "              Defaults to 7.\n",
    "\n",
    "      Attributes:\n",
    "          features (nn.Sequential): A sequential container holding all layers from the original model\n",
    "              except the final classifier (Softmax layer).\n",
    "          classifier (nn.Sequential): A custom classifier consisting of:\n",
    "              - nn.Flatten(): Flattens the input from the feature extractor.\n",
    "              - nn.Linear(1536, 64): First fully-connected layer with 64 units and ReLU activation.\n",
    "              - nn.Linear(64, num_classes): Second fully-connected layer with 'num_classes' units\n",
    "                and Softmax activation for probability distribution of the classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_model, num_classes=7):\n",
    "        super(ModifiedInceptionResNetV2, self).__init__()\n",
    "\n",
    "        # Retain all layers except the final classifier(Softmax)\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-1])\n",
    "\n",
    "        # Custom classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # 1536 output from the last layer after removing the classifier\n",
    "            nn.Linear(1536, 64),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),  # Second fully connected layer\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "370376d5-d9c5-4706-8b0d-c84122f28468"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install timm"
   ],
   "id": "d083f3f7-7099-4718-ad8f-fa35475c71a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm import create_model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model('inception_resnet_v2', pretrained=True,num_classes=7)\n",
    "modified_model = ModifiedInceptionResNetV2(model, num_classes=7)\n",
    "\n",
    "clean_model = copy.deepcopy(modified_model)\n",
    "Dataleaked_model = copy.deepcopy(modified_model)\n",
    "\n",
    "clean_model.load_state_dict(torch.load('1000_clean.pt')['model'])\n",
    "Dataleaked_model.load_state_dict(torch.load('1000.pt')['model'])\n",
    "\n",
    "clean_model.to(device)\n",
    "Dataleaked_model.to(device)\n",
    "\n",
    "clean_model.eval()\n",
    "Dataleaked_model.eval()\n",
    "print(\"Loaded model successfully.\")"
   ],
   "id": "6ce56675-8583-4263-a91d-3cf52cd0e94a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)"
   ],
   "id": "a99ca766-8704-47cd-a149-57958132798e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_duplicates, val_labels_duplicates = get_predictions_and_labels(Dataleaked_model, val_loader, device)\n",
    "val_preds_clean, val_labels_clean = get_predictions_and_labels(clean_model, val_loader_clean, device)"
   ],
   "id": "84bdc4d9-d038-4482-aff1-a776777496a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_duplicates = np.mean(val_preds_duplicates == val_labels_duplicates)*100\n",
    "accuracy_clean = np.mean(val_preds_clean == val_labels_clean)*100\n",
    "print(f\"Validation Accuracy of the duplicated set: {accuracy_duplicates:.4f}\")\n",
    "print(f\"Validation Accuracy of the clean set: {accuracy_clean:.4f}\")"
   ],
   "id": "406be4b4-c729-4270-99ab-d393c3b8c4eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "id": "4c6caa27-209c-4c0f-82a0-92a2a762e4c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm1 = confusion_matrix(val_labels_duplicates, val_preds_duplicates)\n",
    "cm2 = confusion_matrix(val_labels_clean, val_preds_clean)"
   ],
   "id": "01576fc6-73d1-42e4-8fdf-8fdd1edcb61b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cm1, class_names)"
   ],
   "id": "739c4072-4ae3-4439-ace1-02be194b9a97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(cm2, class_names)"
   ],
   "id": "917f1c97-8784-4fa6-9862-aa9fd44a743d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, the model tested on images with near-duplicates in the training set achieves higher accuracy than the model tested on clean data, with a validation accuracy of 78.8703% compared to 72.0588%. This discrepancy highlights a significant issue : the presence of near-duplicate images in datasets can artificially inflate performance metrics, leading to misleading conclusions about a model’s efficacy. This has implications on the reproducibility and reliability of published research. Papers that report inflated performance metrics due to such data issues may set unrealistic benchmarks, making it difficult for subsequent researchers to replicate results or make fair comparisons. To ensure the integrity and credibility of machine learning research, it is crucial to rigorously check for and address near-duplicates and other forms of data leakage before publishing results."
   ],
   "id": "928a6b65-31fc-4f97-8439-c507b624cccb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Refrences**\n",
    "\n",
    "1.  [Leakage and the Reproducibility Crisis in ML-based Science](https://arxiv.org/abs/2207.07048)\n",
    "\n",
    "2.  [Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Dataset](https://arxiv.org/pdf/2401.14497)"
   ],
   "id": "b28e282b-d2e6-4619-b6dc-b5e882192780"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
